import regex
from abc import ABC
from dataclasses import dataclass
from collections import defaultdict
import random

from execute_util import link, image, text
from lecture_util import article_link, x_link, youtube_link
from references import gpt_3, gpt4, shannon1950, bengio2003, susketver2014, \
    bahdanau2015_attention, transformer_2017, gpt2, t5, kaplan_scaling_laws_2020, \
    the_pile, gpt_j, opt_175b, bloom, palm, chinchilla, llama, mistral_7b, \
    instruct_gpt, dpo, adamw2017, lima, deepseek_v3, adam2014, grpo, ppo2017, muon, \
    large_batch_training_2018, wsd_2024, cosine_learning_rate_2017, olmo_7b, moe_2017, \
    megatron_lm_2019, shazeer_2020, elmo, bert, qwen_2_5, deepseek_r1, moe_2017, \
    rms_norm_2019, rope_2021, soap, gqa, mla, deepseek_67b, deepseek_v2, brants2007, \
    layernorm_2016, pre_post_norm_2020, llama2, llama3, olmo2, \
    megabyte, byt5, blt, tfree, sennrich_2016, zero_2019, gpipe_2018
from data import get_common_crawl_urls, read_common_crawl, write_documents, markdownify_documents
from model_util import query_gpt4o

import tiktoken

def main():
    welcome()
    why_this_course_exists()
    current_landscape()

    what_is_this_program()

    course_logistics()
    course_components()

    tokenization()

    text("ä¸‹æ¬¡è¯¾ç¨‹ï¼šPyTorch æ„å»ºæ¨¡å—ï¼Œèµ„æºæ ¸ç®—")


def welcome():
    text("## CS336: ä»é›¶å¼€å§‹æ„å»ºè¯­è¨€æ¨¡å‹ (2025å¹´æ˜¥å­£)"),

    image("images/course-staff.png", width=600)

    text("è¿™æ˜¯ CS336 çš„ç¬¬äºŒæ¬¡å¼€è¯¾ã€‚")
    text("æ–¯å¦ç¦ç‰ˆæœ¬å·²ç»å¢é•¿äº† 50%ã€‚")
    text("è®²åº§å°†å‘å¸ƒåœ¨ YouTube ä¸Šï¼Œå¹¶å‘å…¨ä¸–ç•Œå¼€æ”¾ã€‚")


def why_this_course_exists():
    text("## ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦å¼€è®¾è¿™é—¨è¯¾ç¨‹ï¼Ÿ")

    text("è®©æˆ‘ä»¬é—®é—® GPT-4 "), link(gpt4)
    response = query_gpt4o(prompt="Why teach a course on building language models from scratch? Answer in one sentence.")  # @inspect response
    
    text("é—®é¢˜ï¼šç ”ç©¶äººå‘˜æ­£åœ¨ä¸åº•å±‚æŠ€æœ¯**è„±èŠ‚**ã€‚")
    text("8 å¹´å‰ï¼Œç ”ç©¶äººå‘˜ä¼šå®ç°å¹¶è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚")
    text("6 å¹´å‰ï¼Œç ”ç©¶äººå‘˜ä¼šä¸‹è½½ä¸€ä¸ªæ¨¡å‹ï¼ˆä¾‹å¦‚ BERTï¼‰å¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚")
    text("ä»Šå¤©ï¼Œç ”ç©¶äººå‘˜åªæ˜¯æç¤ºä¸€ä¸ªä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-4/Claude/Geminiï¼‰ã€‚")

    text("æå‡æŠ½è±¡å±‚æ¬¡å¯ä»¥æé«˜ç”Ÿäº§åŠ›ï¼Œä½†æ˜¯")
    text("- è¿™äº›æŠ½è±¡æ˜¯æœ‰æ¼æ´çš„ï¼ˆä¸ç¼–ç¨‹è¯­è¨€æˆ–æ“ä½œç³»ç»Ÿç›¸æ¯”ï¼‰ã€‚")
    text("- ä»ç„¶æœ‰éœ€è¦æ·±å…¥åº•å±‚çš„åŸºç¡€ç ”ç©¶å·¥ä½œè¦åšã€‚")

    text("**å…¨é¢ç†è§£**è¿™é¡¹æŠ€æœ¯å¯¹äº**åŸºç¡€ç ”ç©¶**æ˜¯å¿…è¦çš„ã€‚")

    text("æœ¬è¯¾ç¨‹ï¼š**é€šè¿‡æ„å»ºæ¥ç†è§£**")
    text("ä½†æœ‰ä¸€ä¸ªå°é—®é¢˜...")

    text("## è¯­è¨€æ¨¡å‹çš„å·¥ä¸šåŒ–")
    image("https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Industrialisation.jpg/440px-Industrialisation.jpg", width=400)

    text("æ®ç§° GPT-4 æœ‰ 1.8T å‚æ•°ã€‚"), article_link("https://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-says")
    text("æ®ç§° GPT-4 çš„è®­ç»ƒæˆæœ¬ä¸º 1 äº¿ç¾å…ƒã€‚"), article_link("https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/")
    text("xAI æ„å»ºäº†æ‹¥æœ‰ 200,000 ä¸ª H100 çš„é›†ç¾¤æ¥è®­ç»ƒ Grokã€‚"), article_link("https://www.tomshardware.com/pc-components/gpus/elon-musk-is-doubling-the-worlds-largest-ai-gpu-cluster-expanding-colossus-gpu-cluster-to-200-000-soon-has-floated-300-000-in-the-past")
    text("Stargateï¼ˆOpenAIã€NVIDIAã€Oracleï¼‰åœ¨ 4 å¹´å†…æŠ•èµ„ 5000 äº¿ç¾å…ƒã€‚"), article_link("https://openai.com/index/announcing-the-stargate-project/")

    text("æ­¤å¤–ï¼Œå‰æ²¿æ¨¡å‹çš„æ„å»ºæ–¹å¼æ²¡æœ‰å…¬å¼€ç»†èŠ‚ã€‚")
    text("æ¥è‡ª GPT-4 æŠ€æœ¯æŠ¥å‘Š "), link(gpt4), text("ï¼š")
    image("images/gpt4-no-details.png", width=600)

    text("## è§„æ¨¡ä¸åŒï¼Œæ€§è´¨ä¸åŒ")
    text("å‰æ²¿æ¨¡å‹å¯¹æˆ‘ä»¬æ¥è¯´é¥ä¸å¯åŠã€‚")
    text("ä½†æ„å»ºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆæœ¬è¯¾ç¨‹ä¸­ <1B å‚æ•°ï¼‰å¯èƒ½æ— æ³•ä»£è¡¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚")

    text("ç¤ºä¾‹ 1ï¼šattention ä¸ MLP ä¸­èŠ±è´¹çš„ FLOPs æ¯”ä¾‹éšè§„æ¨¡å˜åŒ–ã€‚"), x_link("https://x.com/stephenroller/status/1579993017234382849")
    image("images/roller-flops.png", width=400)
    text("ç¤ºä¾‹ 2ï¼šéšè§„æ¨¡å‡ºç°çš„æ¶Œç°è¡Œä¸º "), link("https://arxiv.org/pdf/2206.07682")
    image("images/wei-emergence-plot.png", width=600)

    text("## åœ¨è¿™é—¨è¯¾ä¸­æˆ‘ä»¬èƒ½å­¦åˆ°ä»€ä¹ˆå¯ä»¥è¿ç§»åˆ°å‰æ²¿æ¨¡å‹ï¼Ÿ")
    text("æœ‰ä¸‰ç§ç±»å‹çš„çŸ¥è¯†ï¼š")
    text("- **æœºåˆ¶ï¼ˆMechanicsï¼‰**ï¼šäº‹ç‰©å¦‚ä½•å·¥ä½œï¼ˆä»€ä¹ˆæ˜¯ Transformerï¼Œæ¨¡å‹å¹¶è¡Œå¦‚ä½•åˆ©ç”¨ GPUï¼‰")
    text("- **æ€ç»´æ–¹å¼ï¼ˆMindsetï¼‰**ï¼šå……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼Œè®¤çœŸå¯¹å¾…è§„æ¨¡ï¼ˆscaling lawsï¼‰")
    text("- **ç›´è§‰ï¼ˆIntuitionsï¼‰**ï¼šå“ªäº›æ•°æ®å’Œå»ºæ¨¡å†³ç­–èƒ½äº§ç”Ÿè‰¯å¥½çš„å‡†ç¡®æ€§")

    text("æˆ‘ä»¬å¯ä»¥æ•™æˆæœºåˆ¶å’Œæ€ç»´æ–¹å¼ï¼ˆè¿™äº›å¯ä»¥è¿ç§»ï¼‰ã€‚")
    text("æˆ‘ä»¬åªèƒ½éƒ¨åˆ†æ•™æˆç›´è§‰ï¼ˆä¸ä¸€å®šèƒ½è·¨è§„æ¨¡è¿ç§»ï¼‰ã€‚")

    text("## ç›´è§‰ï¼ŸğŸ¤·")
    text("æœ‰äº›è®¾è®¡å†³ç­–ï¼ˆç›®å‰ï¼‰æ— æ³•è¯æ˜åˆç†æ€§ï¼Œåªæ˜¯æ¥è‡ªå®éªŒã€‚")
    text("ç¤ºä¾‹ï¼šNoam Shazeer å¼•å…¥ SwiGLU çš„è®ºæ–‡ "), link(shazeer_2020)
    image("images/divine-benevolence.png", width=600)

    text("## ç—›è‹¦çš„æ•™è®­ï¼ˆThe bitter lessonï¼‰")
    text("é”™è¯¯çš„ç†è§£ï¼šè§„æ¨¡å°±æ˜¯ä¸€åˆ‡ï¼Œç®—æ³•ä¸é‡è¦ã€‚")
    text("æ­£ç¡®çš„ç†è§£ï¼šèƒ½å¤Ÿæ‰©å±•çš„ç®—æ³•æ‰æ˜¯é‡è¦çš„ã€‚")
    text("### å‡†ç¡®æ€§ = æ•ˆç‡ Ã— èµ„æº")
    text("äº‹å®ä¸Šï¼Œåœ¨æ›´å¤§è§„æ¨¡ä¸‹æ•ˆç‡æ›´åŠ é‡è¦ï¼ˆä¸èƒ½æµªè´¹ï¼‰ã€‚")
    link("https://arxiv.org/abs/2005.04305"), text(" æ˜¾ç¤ºåœ¨ 2012 åˆ° 2019 å¹´é—´ï¼ŒImageNet ä¸Šçš„ç®—æ³•æ•ˆç‡æé«˜äº† 44 å€")

    text("æ¡†æ¶ï¼šåœ¨ç»™å®šçš„è®¡ç®—å’Œæ•°æ®é¢„ç®—ä¸‹ï¼Œèƒ½æ„å»ºçš„æœ€ä½³æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ")
    text("æ¢å¥è¯è¯´ï¼Œ**æœ€å¤§åŒ–æ•ˆç‡**ï¼")


def current_landscape():
    text("## ç¥ç»ç½‘ç»œä¹‹å‰ï¼ˆ2010å¹´ä»£ä¹‹å‰ï¼‰")
    text("- ç”¨äºæµ‹é‡è‹±è¯­ç†µçš„è¯­è¨€æ¨¡å‹ "), link(shannon1950)
    text("- å¤§é‡å…³äº n-gram è¯­è¨€æ¨¡å‹çš„å·¥ä½œï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ï¼‰"), link(brants2007)

    text("## ç¥ç»ç½‘ç»œç»„ä»¶ï¼ˆ2010å¹´ä»£ï¼‰")
    text("- ç¬¬ä¸€ä¸ªç¥ç»è¯­è¨€æ¨¡å‹ "), link(bengio2003)
    text("- Sequence-to-sequence å»ºæ¨¡ï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ï¼‰"), link(susketver2014)
    text("- Adam ä¼˜åŒ–å™¨ "), link(adam2014)
    text("- Attention æœºåˆ¶ï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ï¼‰"), link(bahdanau2015_attention)
    text("- Transformer æ¶æ„ï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ï¼‰"), link(transformer_2017)
    text("- Mixture of experts "), link(moe_2017)
    text("- æ¨¡å‹å¹¶è¡Œ "), link(gpipe_2018), link(zero_2019), link(megatron_lm_2019)

    text("## æ—©æœŸåŸºç¡€æ¨¡å‹ï¼ˆ2010å¹´ä»£æœ«ï¼‰")
    text("- ELMoï¼šä½¿ç”¨ LSTM é¢„è®­ç»ƒï¼Œå¾®è°ƒæœ‰åŠ©äºä»»åŠ¡ "), link(elmo)
    text("- BERTï¼šä½¿ç”¨ Transformer é¢„è®­ç»ƒï¼Œå¾®è°ƒæœ‰åŠ©äºä»»åŠ¡ "), link(bert)
    text("- Google çš„ T5 (11B)ï¼šå°†æ‰€æœ‰ä»»åŠ¡è½¬æ¢ä¸º text-to-text "), link(t5)

    text("## æ‹¥æŠ±è§„æ¨¡ï¼Œæ›´åŠ å°é—­")
    text("- OpenAI çš„ GPT-2 (1.5B)ï¼šæµç•…çš„æ–‡æœ¬ï¼Œé¦–æ¬¡å‡ºç° zero-shot è¿¹è±¡ï¼Œåˆ†é˜¶æ®µå‘å¸ƒ "), link(gpt2)
    text("- Scaling lawsï¼šä¸ºæ‰©å±•æä¾›å¸Œæœ›/å¯é¢„æµ‹æ€§ "), link(kaplan_scaling_laws_2020)
    text("- OpenAI çš„ GPT-3 (175B)ï¼šin-context learningï¼Œå°é—­ "), link(gpt_3)
    text("- Google çš„ PaLM (540B)ï¼šå¤§è§„æ¨¡ï¼Œè®­ç»ƒä¸è¶³ "), link(palm)
    text("- DeepMind çš„ Chinchilla (70B)ï¼šè®¡ç®—æœ€ä¼˜ scaling laws "), link(chinchilla)

    text("## å¼€æ”¾æ¨¡å‹")
    text("- EleutherAI çš„å¼€æ”¾æ•°æ®é›†ï¼ˆThe Pileï¼‰å’Œæ¨¡å‹ï¼ˆGPT-Jï¼‰"), link(the_pile), link(gpt_j)
    text("- Meta çš„ OPT (175B)ï¼šGPT-3 å¤ç°ï¼Œè®¸å¤šç¡¬ä»¶é—®é¢˜ "), link(opt_175b)
    text("- Hugging Face / BigScience çš„ BLOOMï¼šä¸“æ³¨äºæ•°æ®æ¥æº "), link(bloom)
    text("- Meta çš„ Llama æ¨¡å‹ "), link(llama), link(llama2), link(llama3)
    text("- é˜¿é‡Œå·´å·´çš„ Qwen æ¨¡å‹ "), link(qwen_2_5)
    text("- DeepSeek çš„æ¨¡å‹ "), link(deepseek_67b), link(deepseek_v2), link(deepseek_v3)
    text("- AI2 çš„ OLMo 2 "), link(olmo_7b), link(olmo2),

    text("## å¼€æ”¾ç¨‹åº¦")
    text("- å°é—­æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-4oï¼‰ï¼šä»… API è®¿é—® "), link(gpt4)
    text("- å¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆä¾‹å¦‚ DeepSeekï¼‰ï¼šæƒé‡å¯ç”¨ï¼Œè®ºæ–‡åŒ…å«æ¶æ„ç»†èŠ‚ï¼Œä¸€äº›è®­ç»ƒç»†èŠ‚ï¼Œæ— æ•°æ®ç»†èŠ‚ "), link(deepseek_v3)
    text("- å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚ OLMoï¼‰ï¼šæƒé‡å’Œæ•°æ®å¯ç”¨ï¼Œè®ºæ–‡åŒ…å«å¤§éƒ¨åˆ†ç»†èŠ‚ï¼ˆä½†ä¸ä¸€å®šåŒ…æ‹¬ç†ç”±ã€å¤±è´¥çš„å®éªŒï¼‰"), link(olmo_7b)

    text("## å½“ä»Šçš„å‰æ²¿æ¨¡å‹")
    text("- OpenAI çš„ o3 "), link("https://openai.com/index/openai-o3-mini/")
    text("- Anthropic çš„ Claude Sonnet 3.7 "), link("https://www.anthropic.com/news/claude-3-7-sonnet")
    text("- xAI çš„ Grok 3 "), link("https://x.ai/news/grok-3")
    text("- Google çš„ Gemini 2.5 "), link("https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/")
    text("- Meta çš„ Llama 3.3 "), link("https://ai.meta.com/blog/meta-llama-3/")
    text("- DeepSeek çš„ r1 "), link(deepseek_r1)
    text("- é˜¿é‡Œå·´å·´çš„ Qwen 2.5 Max "), link("https://qwenlm.github.io/blog/qwen2.5-max/")
    text("- è…¾è®¯çš„ Hunyuan-T1 "), link("https://tencent.github.io/llm.hunyuan.T1/README_EN.html")


def what_is_this_program():
    text("è¿™æ˜¯ä¸€ä¸ª*å¯æ‰§è¡Œè®²åº§*ï¼Œä¸€ä¸ªé€šè¿‡æ‰§è¡Œæ¥ä¼ é€’è®²åº§å†…å®¹çš„ç¨‹åºã€‚")
    text("å¯æ‰§è¡Œè®²åº§ä½¿ä»¥ä¸‹æ“ä½œæˆä¸ºå¯èƒ½ï¼š")
    text("- æŸ¥çœ‹å’Œè¿è¡Œä»£ç ï¼ˆå› ä¸ºä¸€åˆ‡éƒ½æ˜¯ä»£ç ï¼ï¼‰ï¼Œ")
    total = 0  # @inspect total
    for x in [1, 2, 3]:  # @inspect x
        total += x  # @inspect total
    text("- æŸ¥çœ‹è®²åº§çš„å±‚æ¬¡ç»“æ„ï¼Œä»¥åŠ")
    text("- è·³è½¬åˆ°å®šä¹‰å’Œæ¦‚å¿µï¼š"), link(supervised_finetuning)


def course_logistics():
    text("æ‰€æœ‰ä¿¡æ¯éƒ½åœ¨çº¿ä¸Šï¼š"), link("https://stanford-cs336.github.io/spring2025/")

    text("è¿™æ˜¯ä¸€é—¨ 5 å­¦åˆ†çš„è¯¾ç¨‹ã€‚")
    text("æ¥è‡ª 2024 å¹´æ˜¥å­£è¯¾ç¨‹è¯„ä¼°çš„è¯„è®ºï¼š*æ•´ä¸ªä½œä¸šçš„å·¥ä½œé‡å¤§çº¦ç›¸å½“äº CS 224n çš„å…¨éƒ¨ 5 ä¸ªä½œä¸šåŠ ä¸Šæœ€ç»ˆé¡¹ç›®ã€‚è€Œè¿™åªæ˜¯ç¬¬ä¸€ä¸ªä½œä¸šã€‚*")

    text("## ä¸ºä»€ä¹ˆä½ åº”è¯¥é€‰è¿™é—¨è¯¾")
    text("- ä½ æœ‰å¼ºçƒˆçš„éœ€æ±‚å»ç†è§£äº‹ç‰©çš„å·¥ä½œåŸç†ã€‚")
    text("- ä½ æƒ³é”»ç‚¼ç ”ç©¶å·¥ç¨‹èƒ½åŠ›ã€‚")

    text("## ä¸ºä»€ä¹ˆä½ ä¸åº”è¯¥é€‰è¿™é—¨è¯¾")
    text("- ä½ å®é™…ä¸Šæƒ³åœ¨æœ¬å­£åº¦å®Œæˆç ”ç©¶å·¥ä½œã€‚<br>ï¼ˆå’Œä½ çš„å¯¼å¸ˆè°ˆè°ˆã€‚ï¼‰")
    text("- ä½ å¯¹å­¦ä¹  AI ä¸­æœ€çƒ­é—¨çš„æ–°æŠ€æœ¯æ„Ÿå…´è¶£ï¼ˆä¾‹å¦‚å¤šæ¨¡æ€ã€RAG ç­‰ï¼‰ã€‚<br>ï¼ˆä½ åº”è¯¥é€‰ä¸€é—¨ç ”è®¨è¯¾ã€‚ï¼‰")
    text("- ä½ æƒ³åœ¨è‡ªå·±çš„åº”ç”¨é¢†åŸŸè·å¾—è‰¯å¥½ç»“æœã€‚<br>ï¼ˆä½ åº”è¯¥åªéœ€æç¤ºæˆ–å¾®è°ƒç°æœ‰æ¨¡å‹ã€‚ï¼‰")

    text("## å¦‚ä½•åœ¨å®¶è·Ÿéšå­¦ä¹ ")
    text("- æ‰€æœ‰è®²åº§ææ–™å’Œä½œä¸šéƒ½å°†åœ¨çº¿å‘å¸ƒï¼Œæ‰€ä»¥å¯ä»¥è‡ªç”±è·Ÿéšå­¦ä¹ ã€‚")
    text("- è®²åº§é€šè¿‡ [CGOEï¼Œæ­£å¼åç§° SCPD](https://cgoe.stanford.edu/) å½•åˆ¶ï¼Œå¹¶åœ¨ YouTube ä¸Šæä¾›ï¼ˆä¼šæœ‰ä¸€äº›å»¶è¿Ÿï¼‰ã€‚")
    text("- æˆ‘ä»¬è®¡åˆ’æ˜å¹´å†æ¬¡å¼€è®¾è¿™é—¨è¯¾ã€‚")

    text("## ä½œä¸š")
    text("- 5 ä¸ªä½œä¸šï¼ˆåŸºç¡€ã€ç³»ç»Ÿã€scaling lawsã€æ•°æ®ã€å¯¹é½ï¼‰ã€‚")
    text("- æ²¡æœ‰è„šæ‰‹æ¶ä»£ç ï¼Œä½†æˆ‘ä»¬æä¾›å•å…ƒæµ‹è¯•å’Œé€‚é…å™¨æ¥å£æ¥å¸®åŠ©ä½ æ£€æŸ¥æ­£ç¡®æ€§ã€‚")
    text("- åœ¨æœ¬åœ°å®ç°ä»¥æµ‹è¯•æ­£ç¡®æ€§ï¼Œç„¶ååœ¨é›†ç¾¤ä¸Šè¿è¡Œä»¥è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼‰ã€‚")
    text("- æŸäº›ä½œä¸šæœ‰æ’è¡Œæ¦œï¼ˆåœ¨ç»™å®šè®­ç»ƒé¢„ç®—ä¸‹æœ€å°åŒ–å›°æƒ‘åº¦ï¼‰ã€‚")
    text("- AI å·¥å…·ï¼ˆä¾‹å¦‚ CoPilotã€Cursorï¼‰å¯èƒ½ä¼šå½±å“å­¦ä¹ ï¼Œæ‰€ä»¥ä½¿ç”¨æ—¶éœ€è‡ªæ‹…é£é™©ã€‚")

    text("## é›†ç¾¤")
    text("- æ„Ÿè°¢ Together AI æä¾›è®¡ç®—é›†ç¾¤ã€‚ğŸ™")
    text("- è¯·é˜…è¯»[æŒ‡å—](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit)äº†è§£å¦‚ä½•ä½¿ç”¨é›†ç¾¤ã€‚")
    text("- å°½æ—©å¼€å§‹ä½œä¸šï¼Œå› ä¸ºä¸´è¿‘æˆªæ­¢æ—¥æœŸæ—¶é›†ç¾¤ä¼šè¢«å æ»¡ï¼")


def course_components():
    text("## ä¸€åˆ‡éƒ½å…³ä¹æ•ˆç‡")
    text("èµ„æºï¼šæ•°æ® + ç¡¬ä»¶ï¼ˆè®¡ç®—ã€å†…å­˜ã€é€šä¿¡å¸¦å®½ï¼‰")
    text("åœ¨ç»™å®šçš„å›ºå®šèµ„æºé›†ä¸‹ï¼Œå¦‚ä½•è®­ç»ƒæœ€ä½³æ¨¡å‹ï¼Ÿ")
    text("ç¤ºä¾‹ï¼šç»™å®šä¸€ä¸ª Common Crawl è½¬å‚¨å’Œ 32 ä¸ª H100ï¼ŒæŒç»­ 2 å‘¨ï¼Œä½ åº”è¯¥æ€ä¹ˆåšï¼Ÿ")

    text("è®¾è®¡å†³ç­–ï¼š")
    image("images/design-decisions.png", width=800)

    text("## è¯¾ç¨‹æ¦‚è§ˆ")
    basics()
    systems()
    scaling_laws()
    data()
    alignment()

    text("## æ•ˆç‡é©±åŠ¨è®¾è®¡å†³ç­–")

    text("ä»Šå¤©ï¼Œæˆ‘ä»¬å—è®¡ç®—çº¦æŸï¼Œå› æ­¤è®¾è®¡å†³ç­–å°†åæ˜ å¦‚ä½•å……åˆ†åˆ©ç”¨ç»™å®šç¡¬ä»¶ã€‚")
    text("- æ•°æ®å¤„ç†ï¼šé¿å…åœ¨ç³Ÿç³•/æ— å…³çš„æ•°æ®ä¸Šæµªè´¹å®è´µçš„è®¡ç®—èµ„æº")
    text("- Tokenizationï¼šä½¿ç”¨åŸå§‹å­—èŠ‚å¾ˆä¼˜é›…ï¼Œä½†åœ¨å½“ä»Šçš„æ¨¡å‹æ¶æ„ä¸‹è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚")
    text("- æ¨¡å‹æ¶æ„ï¼šè®¸å¤šå˜åŒ–æ˜¯ä¸ºäº†å‡å°‘å†…å­˜æˆ– FLOPsï¼ˆä¾‹å¦‚å…±äº« KV ç¼“å­˜ã€æ»‘åŠ¨çª—å£ attentionï¼‰")
    text("- è®­ç»ƒï¼šæˆ‘ä»¬å¯ä»¥åªç”¨ä¸€ä¸ª epochï¼")
    text("- Scaling lawsï¼šåœ¨è¾ƒå°æ¨¡å‹ä¸Šä½¿ç”¨æ›´å°‘è®¡ç®—æ¥è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
    text("- å¯¹é½ï¼šå¦‚æœå°†æ¨¡å‹æ›´å¤šåœ°è°ƒæ•´åˆ°æ‰€éœ€ç”¨ä¾‹ï¼Œåˆ™éœ€è¦æ›´å°çš„åŸºç¡€æ¨¡å‹")

    text("æ˜å¤©ï¼Œæˆ‘ä»¬å°†å—åˆ°æ•°æ®çº¦æŸ...")


class Tokenizer(ABC):
    """Tokenizer çš„æŠ½è±¡æ¥å£ã€‚"""
    def encode(self, string: str) -> list[int]:
        raise NotImplementedError

    def decode(self, indices: list[int]) -> str:
        raise NotImplementedError


def basics():
    text("ç›®æ ‡ï¼šè®©å®Œæ•´æµæ°´çº¿çš„åŸºæœ¬ç‰ˆæœ¬è¿è¡Œèµ·æ¥")
    text("ç»„ä»¶ï¼štokenizationã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒ")

    text("## Tokenization")
    text("Tokenizer åœ¨å­—ç¬¦ä¸²å’Œæ•´æ•°åºåˆ—ï¼ˆtokenï¼‰ä¹‹é—´è¿›è¡Œè½¬æ¢")
    image("images/tokenized-example.png", width=600) 
    text("ç›´è§‰ï¼šå°†å­—ç¬¦ä¸²åˆ†è§£ä¸ºå¸¸è§ç‰‡æ®µ")

    text("æœ¬è¯¾ç¨‹ï¼šByte-Pair Encoding (BPE) tokenizer "), link(sennrich_2016)

    text("æ—  tokenizer æ–¹æ³•ï¼š"), link(byt5), link(megabyte), link(blt), link(tfree)
    text("ç›´æ¥ä½¿ç”¨å­—èŠ‚ï¼Œå¾ˆæœ‰å‰æ™¯ï¼Œä½†å°šæœªæ‰©å±•åˆ°å‰æ²¿æ°´å¹³ã€‚")
    
    text("## æ¶æ„")
    text("èµ·ç‚¹ï¼šåŸå§‹ Transformer "), link(transformer_2017)
    image("images/transformer-architecture.png", width=500)

    text("å˜ä½“ï¼š")
    text("- æ¿€æ´»å‡½æ•°ï¼šReLUã€SwiGLU "), link(shazeer_2020)
    text("- ä½ç½®ç¼–ç ï¼šsinusoidalã€RoPE "), link(rope_2021)
    text("- å½’ä¸€åŒ–ï¼šLayerNormã€RMSNorm "), link(layernorm_2016), link(rms_norm_2019)
    text("- å½’ä¸€åŒ–çš„ä½ç½®ï¼špre-norm ä¸ post-norm "), link(pre_post_norm_2020)
    text("- MLPï¼šdenseã€mixture of experts "), link(moe_2017)
    text("- Attentionï¼šfullã€sliding windowã€linear "), link(mistral_7b), link("https://arxiv.org/abs/2006.16236")
    text("- ä½ç»´ attentionï¼šgroup-query attention (GQA)ã€multi-head latent attention (MLA) "), link(gqa), link(mla)
    text("- çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼šHyena "), link("https://arxiv.org/abs/2302.10866")

    text("## è®­ç»ƒ")
    text("- ä¼˜åŒ–å™¨ï¼ˆä¾‹å¦‚ AdamWã€Muonã€SOAPï¼‰"), link(adam2014), link(adamw2017), link(muon), link(soap)
    text("- å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¾‹å¦‚ cosineã€WSDï¼‰"), link(cosine_learning_rate_2017), link(wsd_2024)
    text("- Batch sizeï¼ˆä¾‹å¦‚ä¸´ç•Œ batch sizeï¼‰"), link(large_batch_training_2018)
    text("- æ­£åˆ™åŒ–ï¼ˆä¾‹å¦‚ dropoutã€weight decayï¼‰")
    text("- è¶…å‚æ•°ï¼ˆhead æ•°é‡ã€éšè—ç»´åº¦ï¼‰ï¼šç½‘æ ¼æœç´¢")

    text("## ä½œä¸š 1")
    link(title="[GitHub]", url="https://github.com/stanford-cs336/assignment1-basics"), link(title="[PDF]", url="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf")
    text("- å®ç° BPE tokenizer")
    text("- å®ç° Transformerã€äº¤å‰ç†µæŸå¤±ã€AdamW ä¼˜åŒ–å™¨ã€è®­ç»ƒå¾ªç¯")
    text("- åœ¨ TinyStories å’Œ OpenWebText ä¸Šè®­ç»ƒ")
    text("- æ’è¡Œæ¦œï¼šåœ¨ H100 ä¸Š 90 åˆ†é’Ÿå†…æœ€å°åŒ– OpenWebText å›°æƒ‘åº¦ "), link(title="[å»å¹´çš„æ’è¡Œæ¦œ]", url="https://github.com/stanford-cs336/spring2024-assignment1-basics-leaderboard")


def systems():
    text("ç›®æ ‡ï¼šå……åˆ†åˆ©ç”¨ç¡¬ä»¶")
    text("ç»„ä»¶ï¼škernelã€å¹¶è¡Œã€æ¨ç†")

    text("## Kernel")
    text("GPU (A100) çš„æ ·å­ï¼š")
    image("https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg", width=800)
    text("ç±»æ¯”ï¼šä»“åº“ : DRAM :: å·¥å‚ : SRAM")
    image("https://horace.io/img/perf_intro/factory_bandwidth.png", width=400)
    text("æŠ€å·§ï¼šé€šè¿‡æœ€å°åŒ–æ•°æ®ç§»åŠ¨æ¥ç»„ç»‡è®¡ç®—ï¼Œä»¥æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡")
    text("ä½¿ç”¨ CUDA/**Triton**/CUTLASS/ThunderKittens ç¼–å†™ kernel")

    text("## å¹¶è¡Œ")
    text("å¦‚æœæˆ‘ä»¬æœ‰å¤šä¸ª GPUï¼ˆ8 ä¸ª A100ï¼‰å‘¢ï¼Ÿ")
    image("https://www.fibermall.com/blog/wp-content/uploads/2024/09/the-hardware-topology-of-a-typical-8xA100-GPU-host.png", width=500)
    text("GPU ä¹‹é—´çš„æ•°æ®ç§»åŠ¨æ›´æ…¢ï¼Œä½†åŒæ ·çš„'æœ€å°åŒ–æ•°æ®ç§»åŠ¨'åŸåˆ™ä»ç„¶é€‚ç”¨")
    text("ä½¿ç”¨é›†åˆæ“ä½œï¼ˆä¾‹å¦‚ gatherã€reduceã€all-reduceï¼‰")
    text("è·¨ GPU åˆ†ç‰‡ï¼ˆå‚æ•°ã€æ¿€æ´»ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰")
    text("å¦‚ä½•æ‹†åˆ†è®¡ç®—ï¼š{data, tensor, pipeline, sequence} å¹¶è¡Œ")
    
    text("## æ¨ç†")
    text("ç›®æ ‡ï¼šç»™å®šæç¤ºç”Ÿæˆ tokenï¼ˆå®é™…ä½¿ç”¨æ¨¡å‹æ‰€éœ€ï¼ï¼‰")
    text("æ¨ç†ä¹Ÿéœ€è¦ç”¨äºå¼ºåŒ–å­¦ä¹ ã€æµ‹è¯•æ—¶è®¡ç®—ã€è¯„ä¼°")
    text("å…¨çƒèŒƒå›´å†…ï¼Œæ¨ç†è®¡ç®—ï¼ˆæ¯æ¬¡ä½¿ç”¨ï¼‰è¶…è¿‡è®­ç»ƒè®¡ç®—ï¼ˆä¸€æ¬¡æ€§æˆæœ¬ï¼‰")
    text("ä¸¤ä¸ªé˜¶æ®µï¼šprefill å’Œ decode")
    image("images/prefill-decode.png", width=500)
    text("Prefillï¼ˆç±»ä¼¼äºè®­ç»ƒï¼‰ï¼štoken å·²ç»™å®šï¼Œå¯ä»¥ä¸€æ¬¡å¤„ç†æ‰€æœ‰ï¼ˆè®¡ç®—å—é™ï¼‰")
    text("Decodeï¼šéœ€è¦ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª tokenï¼ˆå†…å­˜å—é™ï¼‰")
    text("åŠ é€Ÿè§£ç çš„æ–¹æ³•ï¼š")
    text("- ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼ˆé€šè¿‡æ¨¡å‹å‰ªæã€é‡åŒ–ã€è’¸é¦ï¼‰")
    text("- Speculative decodingï¼šä½¿ç”¨æ›´ä¾¿å®œçš„\"è‰ç¨¿\"æ¨¡å‹ç”Ÿæˆå¤šä¸ª tokenï¼Œç„¶åä½¿ç”¨å®Œæ•´æ¨¡å‹å¹¶è¡Œè¯„åˆ†ï¼ˆç²¾ç¡®è§£ç ï¼ï¼‰")
    text("- ç³»ç»Ÿä¼˜åŒ–ï¼šKV ç¼“å­˜ã€æ‰¹å¤„ç†")

    text("## ä½œä¸š 2")
    link(title="[2024å¹´çš„ GitHub]", url="https://github.com/stanford-cs336/spring2024-assignment2-systems"), link(title="[2024å¹´çš„ PDF]", url="https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf")
    text("- åœ¨ Triton ä¸­å®ç°èåˆçš„ RMSNorm kernel")
    text("- å®ç°åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒ")
    text("- å®ç°ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡")
    text("- å¯¹å®ç°è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ€§èƒ½åˆ†æ")


def scaling_laws():
    text("ç›®æ ‡ï¼šåœ¨å°è§„æ¨¡ä¸Šåšå®éªŒï¼Œé¢„æµ‹å¤§è§„æ¨¡çš„è¶…å‚æ•°/æŸå¤±")
    text("é—®é¢˜ï¼šç»™å®š FLOPs é¢„ç®—ï¼ˆ$C$ï¼‰ï¼Œä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆ$N$ï¼‰è¿˜æ˜¯åœ¨æ›´å¤š token ä¸Šè®­ç»ƒï¼ˆ$D$ï¼‰ï¼Ÿ")
    text("è®¡ç®—æœ€ä¼˜ scaling lawsï¼š"), link(kaplan_scaling_laws_2020), link(chinchilla)
    image("images/chinchilla-isoflop.png", width=800)
    text("ç®€è€Œè¨€ä¹‹ï¼š$D^* = 20 N^*$ï¼ˆä¾‹å¦‚ï¼Œ1.4B å‚æ•°æ¨¡å‹åº”è¯¥åœ¨ 28B token ä¸Šè®­ç»ƒï¼‰")
    text("ä½†è¿™æ²¡æœ‰è€ƒè™‘æ¨ç†æˆæœ¬ï¼")

    text("## ä½œä¸š 3")
    link(title="[2024å¹´çš„ GitHub]", url="https://github.com/stanford-cs336/spring2024-assignment3-scaling"), link(title="[2024å¹´çš„ PDF]", url="https://github.com/stanford-cs336/spring2024-assignment3-scaling/blob/master/cs336_spring2024_assignment3_scaling.pdf")
    text("- æˆ‘ä»¬åŸºäºä¹‹å‰çš„è¿è¡Œå®šä¹‰ä¸€ä¸ªè®­ç»ƒ APIï¼ˆè¶…å‚æ•° -> æŸå¤±ï¼‰")
    text("- æäº¤\"è®­ç»ƒä»»åŠ¡\"ï¼ˆåœ¨ FLOPs é¢„ç®—ä¸‹ï¼‰å¹¶æ”¶é›†æ•°æ®ç‚¹")
    text("- å°† scaling law æ‹Ÿåˆåˆ°æ•°æ®ç‚¹")
    text("- æäº¤æ‰©å±•è¶…å‚æ•°çš„é¢„æµ‹")
    text("- æ’è¡Œæ¦œï¼šåœ¨ç»™å®š FLOPs é¢„ç®—ä¸‹æœ€å°åŒ–æŸå¤±")


def data():
    text("é—®é¢˜ï¼šæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å…·æœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ")
    text("å¤šè¯­è¨€ï¼Ÿä»£ç ï¼Ÿæ•°å­¦ï¼Ÿ")
    image("https://ar5iv.labs.arxiv.org/html/2101.00027/assets/pile_chart2.png", width=600)

    text("## è¯„ä¼°")
    text("- å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼šè¯­è¨€æ¨¡å‹çš„æ•™ç§‘ä¹¦å¼è¯„ä¼°")
    text("- æ ‡å‡†åŒ–æµ‹è¯•ï¼ˆä¾‹å¦‚ MMLUã€HellaSwagã€GSM8Kï¼‰")
    text("- æŒ‡ä»¤éµå¾ªï¼ˆä¾‹å¦‚ AlpacaEvalã€IFEvalã€WildBenchï¼‰")
    text("- æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—ï¼šchain-of-thoughtã€é›†æˆ")
    text("- LM-as-a-judgeï¼šè¯„ä¼°ç”Ÿæˆä»»åŠ¡")
    text("- å®Œæ•´ç³»ç»Ÿï¼šRAGã€agent")

    text("## æ•°æ®ç­–åˆ’")
    text("- æ•°æ®ä¸ä¼šä»å¤©è€Œé™ã€‚")
    look_at_web_data()
    text("- æ¥æºï¼šä»äº’è”ç½‘çˆ¬å–çš„ç½‘é¡µã€ä¹¦ç±ã€arXiv è®ºæ–‡ã€GitHub ä»£ç ç­‰ã€‚")
    text("- è¯‰è¯¸åˆç†ä½¿ç”¨æ¥è®­ç»ƒç‰ˆæƒæ•°æ®ï¼Ÿ"), link("https://arxiv.org/pdf/2303.15715.pdf")
    text("- å¯èƒ½éœ€è¦æˆæƒæ•°æ®ï¼ˆä¾‹å¦‚ Google ä¸ Reddit æ•°æ®ï¼‰"), article_link("https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/")
    text("- æ ¼å¼ï¼šHTMLã€PDFã€ç›®å½•ï¼ˆä¸æ˜¯æ–‡æœ¬ï¼ï¼‰")

    text("## æ•°æ®å¤„ç†")
    text("- è½¬æ¢ï¼šå°† HTML/PDF è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆä¿ç•™å†…å®¹ã€ä¸€äº›ç»“æ„ã€é‡å†™ï¼‰")
    text("- è¿‡æ»¤ï¼šä¿ç•™é«˜è´¨é‡æ•°æ®ï¼Œåˆ é™¤æœ‰å®³å†…å®¹ï¼ˆé€šè¿‡åˆ†ç±»å™¨ï¼‰")
    text("- å»é‡ï¼šèŠ‚çœè®¡ç®—ï¼Œé¿å…è®°å¿†ï¼›ä½¿ç”¨ Bloom filter æˆ– MinHash")

    text("## ä½œä¸š 4")
    link(title="[2024å¹´çš„ GitHub]", url="https://github.com/stanford-cs336/spring2024-assignment4-data"), link(title="[2024å¹´çš„ PDF]", url="https://github.com/stanford-cs336/spring2024-assignment4-data/blob/master/cs336_spring2024_assignment4_data.pdf")
    text("- å°† Common Crawl HTML è½¬æ¢ä¸ºæ–‡æœ¬")
    text("- è®­ç»ƒåˆ†ç±»å™¨ä»¥è¿‡æ»¤è´¨é‡å’Œæœ‰å®³å†…å®¹")
    text("- ä½¿ç”¨ MinHash å»é‡")
    text("- æ’è¡Œæ¦œï¼šåœ¨ç»™å®š token é¢„ç®—ä¸‹æœ€å°åŒ–å›°æƒ‘åº¦")


def look_at_web_data():
    urls = get_common_crawl_urls()[:3]  # @inspect urls
    documents = list(read_common_crawl(urls[1], limit=300))
    random.seed(40)
    random.shuffle(documents)
    documents = markdownify_documents(documents[:10])
    write_documents(documents, "var/sample-documents.txt")
    link(title="[ç¤ºä¾‹æ–‡æ¡£]", url="var/sample-documents.txt")
    text("å¤–é¢æ˜¯ä¸€ç‰‡è’åœ°ï¼éœ€è¦çœŸæ­£å¤„ç†æ•°æ®ã€‚")


def alignment():
    text("åˆ°ç›®å‰ä¸ºæ­¢ï¼Œ**åŸºç¡€æ¨¡å‹**æ˜¯åŸå§‹æ½œåŠ›ï¼Œéå¸¸æ“…é•¿å®Œæˆä¸‹ä¸€ä¸ª tokenã€‚")
    text("å¯¹é½ä½¿æ¨¡å‹çœŸæ­£æœ‰ç”¨ã€‚")

    text("å¯¹é½çš„ç›®æ ‡ï¼š")
    text("- è®©è¯­è¨€æ¨¡å‹éµå¾ªæŒ‡ä»¤")
    text("- è°ƒæ•´é£æ ¼ï¼ˆæ ¼å¼ã€é•¿åº¦ã€è¯­æ°”ç­‰ï¼‰")
    text("- çº³å…¥å®‰å…¨æ€§ï¼ˆä¾‹å¦‚æ‹’ç»å›ç­”æœ‰å®³é—®é¢˜ï¼‰")

    text("ä¸¤ä¸ªé˜¶æ®µï¼š")
    supervised_finetuning()
    learning_from_feedback()

    text("## ä½œä¸š 5")
    link(title="[2024å¹´çš„ GitHub]", url="https://github.com/stanford-cs336/spring2024-assignment5-alignment"), link(title="[2024å¹´çš„ PDF]", url="https://github.com/stanford-cs336/spring2024-assignment5-alignment/blob/master/cs336_spring2024_assignment5_alignment.pdf")
    text("- å®ç°ç›‘ç£å¾®è°ƒ")
    text("- å®ç° Direct Preference Optimization (DPO)")
    text("- å®ç° Group Relative Preference Optimization (GRPO)")


@dataclass(frozen=True)
class Turn:
    role: str
    content: str


@dataclass(frozen=True)
class ChatExample:
    turns: list[Turn]


@dataclass(frozen=True)
class PreferenceExample:
    history: list[Turn]
    response_a: str
    response_b: str
    chosen: str


def supervised_finetuning():
    text("## ç›‘ç£å¾®è°ƒï¼ˆSupervised finetuning, SFTï¼‰")

    text("æŒ‡ä»¤æ•°æ®ï¼šï¼ˆæç¤ºï¼Œå“åº”ï¼‰å¯¹")
    sft_data: list[ChatExample] = [
        ChatExample(
            turns=[
                Turn(role="system", content="You are a helpful assistant."),
                Turn(role="user", content="What is 1 + 1?"),
                Turn(role="assistant", content="The answer is 2."),
            ],
        ),
    ]
    text("æ•°æ®é€šå¸¸æ¶‰åŠäººå·¥æ ‡æ³¨ã€‚")
    text("ç›´è§‰ï¼šåŸºç¡€æ¨¡å‹å·²ç»å…·å¤‡æŠ€èƒ½ï¼Œåªéœ€è¦å°‘é‡ç¤ºä¾‹æ¥å±•ç°å®ƒä»¬ã€‚"), link(lima)
    text("ç›‘ç£å­¦ä¹ ï¼šå¾®è°ƒæ¨¡å‹ä»¥æœ€å¤§åŒ– p(response | prompt)ã€‚")


def learning_from_feedback():
    text("ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªåˆæ­¥çš„æŒ‡ä»¤éµå¾ªæ¨¡å‹ã€‚")
    text("è®©æˆ‘ä»¬åœ¨ä¸è¿›è¡Œæ˜‚è´µæ ‡æ³¨çš„æƒ…å†µä¸‹æ”¹è¿›å®ƒã€‚")
    
    text("## åå¥½æ•°æ®")
    text("æ•°æ®ï¼šä½¿ç”¨æ¨¡å‹å¯¹ç»™å®šæç¤ºç”Ÿæˆå¤šä¸ªå“åº”ï¼ˆä¾‹å¦‚ [A, B]ï¼‰ã€‚")
    text("ç”¨æˆ·æä¾›åå¥½ï¼ˆä¾‹å¦‚ A < B æˆ– A > Bï¼‰ã€‚")
    preference_data: list[PreferenceExample] = [
        PreferenceExample(
            history=[
                Turn(role="system", content="You are a helpful assistant."),
                Turn(role="user", content="What is the best way to train a language model?"),
            ],
            response_a="You should use a large dataset and train for a long time.",
            response_b="You should use a small dataset and train for a short time.",
            chosen="a",
        )
    ]

    text("## éªŒè¯å™¨")
    text("- å½¢å¼åŒ–éªŒè¯å™¨ï¼ˆä¾‹å¦‚ç”¨äºä»£ç ã€æ•°å­¦ï¼‰")
    text("- å­¦ä¹ çš„éªŒè¯å™¨ï¼šé’ˆå¯¹ LM-as-a-judge è¿›è¡Œè®­ç»ƒ")

    text("## ç®—æ³•")
    text("- æ¥è‡ªå¼ºåŒ–å­¦ä¹ çš„ Proximal Policy Optimization (PPO) "), link(ppo2017), link(instruct_gpt)
    text("- Direct Policy Optimization (DPO)ï¼šç”¨äºåå¥½æ•°æ®ï¼Œæ›´ç®€å• "), link(dpo)
    text("- Group Relative Preference Optimization (GRPO)ï¼šç§»é™¤ value function "), link(grpo)


############################################################
# Tokenization

# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23
GPT2_TOKENIZER_REGEX = \
    r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""


def tokenization():
    text("æœ¬å•å…ƒå— Andrej Karpathy å…³äº tokenization çš„è§†é¢‘å¯å‘ï¼›å»çœ‹çœ‹å§ï¼"), youtube_link("https://www.youtube.com/watch?v=zduSFxRajkE")

    intro_to_tokenization()
    tokenization_examples()
    character_tokenizer()
    byte_tokenizer()
    word_tokenizer()
    bpe_tokenizer()

    text("## æ€»ç»“")
    text("- Tokenizerï¼šå­—ç¬¦ä¸² <-> tokenï¼ˆç´¢å¼•ï¼‰")
    text("- åŸºäºå­—ç¬¦ã€åŸºäºå­—èŠ‚ã€åŸºäºå•è¯çš„ tokenization é«˜åº¦æ¬¡ä¼˜")
    text("- BPE æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯å‘å¼æ–¹æ³•ï¼ŒæŸ¥çœ‹è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯")
    text("- Tokenization æ˜¯ä¸€ä¸ªå¿…è¦çš„æ¶ï¼Œä¹Ÿè®¸æœ‰ä¸€å¤©æˆ‘ä»¬åªéœ€ä»å­—èŠ‚å¼€å§‹...")

@dataclass(frozen=True)
class BPETokenizerParams:
    """æŒ‡å®š BPETokenizer æ‰€éœ€çš„å…¨éƒ¨å†…å®¹ã€‚"""
    vocab: dict[int, bytes]     # index -> bytes
    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index



class CharacterTokenizer(Tokenizer):
    """å°†å­—ç¬¦ä¸²è¡¨ç¤ºä¸º Unicode ç ç‚¹åºåˆ—ã€‚"""
    def encode(self, string: str) -> list[int]:
        return list(map(ord, string))

    def decode(self, indices: list[int]) -> str:
        return "".join(map(chr, indices))


class ByteTokenizer(Tokenizer):
    """å°†å­—ç¬¦ä¸²è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—ã€‚"""
    def encode(self, string: str) -> list[int]:
        string_bytes = string.encode("utf-8")  # @inspect string_bytes
        indices = list(map(int, string_bytes))  # @inspect indices
        return indices

    def decode(self, indices: list[int]) -> str:
        string_bytes = bytes(indices)  # @inspect string_bytes
        string = string_bytes.decode("utf-8")  # @inspect string
        return string


def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index
    """è¿”å› `indices`ï¼Œä½†å°†æ‰€æœ‰ `pair` å®ä¾‹æ›¿æ¢ä¸º `new_index`ã€‚"""
    new_indices = []  # @inspect new_indices
    i = 0  # @inspect i
    while i < len(indices):
        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:
            new_indices.append(new_index)
            i += 2
        else:
            new_indices.append(indices[i])
            i += 1
    return new_indices


class BPETokenizer(Tokenizer):
    """ç»™å®šä¸€ç»„åˆå¹¶å’Œè¯æ±‡è¡¨çš„ BPE tokenizerã€‚"""
    def __init__(self, params: BPETokenizerParams):
        self.params = params

    def encode(self, string: str) -> list[int]:
        indices = list(map(int, string.encode("utf-8")))  # @inspect indices
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸æ…¢çš„å®ç°
        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index
            indices = merge(indices, pair, new_index)
        return indices

    def decode(self, indices: list[int]) -> str:
        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list
        string = b"".join(bytes_list).decode("utf-8")  # @inspect string
        return string


def get_compression_ratio(string: str, indices: list[int]) -> float:
    """ç»™å®šå·²è¢« tokenize ä¸º `indices` çš„ `string`ï¼Œè®¡ç®—å‹ç¼©æ¯”ã€‚"""
    num_bytes = len(bytes(string, encoding="utf-8"))  # @inspect num_bytes
    num_tokens = len(indices)                       # @inspect num_tokens
    return num_bytes / num_tokens


def get_gpt2_tokenizer():
    # Code: https://github.com/openai/tiktoken
    # You can use cl100k_base for the gpt3.5-turbo or gpt4 tokenizer
    return tiktoken.get_encoding("gpt2")


def intro_to_tokenization():
    text("åŸå§‹æ–‡æœ¬é€šå¸¸è¡¨ç¤ºä¸º Unicode å­—ç¬¦ä¸²ã€‚")
    string = "Hello, ğŸŒ! ä½ å¥½!"

    text("è¯­è¨€æ¨¡å‹åœ¨ token åºåˆ—ä¸Šæ”¾ç½®æ¦‚ç‡åˆ†å¸ƒï¼ˆé€šå¸¸ç”±æ•´æ•°ç´¢å¼•è¡¨ç¤ºï¼‰ã€‚")
    indices = [15496, 11, 995, 0]

    text("æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå°†å­—ç¬¦ä¸²*ç¼–ç *ä¸º token çš„è¿‡ç¨‹ã€‚")
    text("æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå°† token *è§£ç *å›å­—ç¬¦ä¸²çš„è¿‡ç¨‹ã€‚")
    text("ä¸€ä¸ª "), link(Tokenizer), text(" æ˜¯å®ç° encode å’Œ decode æ–¹æ³•çš„ç±»ã€‚")
    text("**è¯æ±‡è¡¨å¤§å°**æ˜¯å¯èƒ½çš„ tokenï¼ˆæ•´æ•°ï¼‰æ•°é‡ã€‚")


def tokenization_examples():
    text("è¦äº†è§£ tokenizer çš„å·¥ä½œåŸç†ï¼Œè¯·ä½¿ç”¨è¿™ä¸ª "), link(title="äº¤äº’å¼ç½‘ç«™", url="https://tiktokenizer.vercel.app/?encoder=gpt2")

    text("## è§‚å¯Ÿ")
    text("- ä¸€ä¸ªå•è¯åŠå…¶å‰é¢çš„ç©ºæ ¼æ˜¯åŒä¸€ä¸ª token çš„ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚ \" world\"ï¼‰ã€‚")
    text("- å¼€å¤´å’Œä¸­é—´çš„å•è¯è¡¨ç¤ºæ–¹å¼ä¸åŒï¼ˆä¾‹å¦‚ \"hello hello\"ï¼‰ã€‚")
    text("- æ•°å­—è¢« tokenize ä¸ºæ¯å‡ ä½æ•°å­—ã€‚")

    text("è¿™æ˜¯æ¥è‡ª OpenAI çš„ GPT-2 tokenizerï¼ˆtiktokenï¼‰çš„å®é™…åº”ç”¨ã€‚")
    tokenizer = get_gpt2_tokenizer()
    string = "Hello, ğŸŒ! ä½ å¥½!"  # @inspect string

    text("æ£€æŸ¥ encode() å’Œ decode() æ˜¯å¦å¾€è¿”ï¼š")
    indices = tokenizer.encode(string)  # @inspect indices
    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
    assert string == reconstructed_string
    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio


def character_tokenizer():
    text("## åŸºäºå­—ç¬¦çš„ tokenization")

    text("Unicode å­—ç¬¦ä¸²æ˜¯ Unicode å­—ç¬¦çš„åºåˆ—ã€‚")
    text("æ¯ä¸ªå­—ç¬¦å¯ä»¥é€šè¿‡ `ord` è½¬æ¢ä¸ºç ç‚¹ï¼ˆæ•´æ•°ï¼‰ã€‚")
    assert ord("a") == 97
    assert ord("ğŸŒ") == 127757
    text("å¯ä»¥é€šè¿‡ `chr` è½¬æ¢å›æ¥ã€‚")
    assert chr(97) == "a"
    assert chr(127757) == "ğŸŒ"

    text("ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª `Tokenizer` å¹¶ç¡®ä¿å®ƒå¾€è¿”ï¼š")
    tokenizer = CharacterTokenizer()
    string = "Hello, ğŸŒ! ä½ å¥½!"  # @inspect string
    indices = tokenizer.encode(string)  # @inspect indices
    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
    assert string == reconstructed_string

    text("å¤§çº¦æœ‰ 150K ä¸ª Unicode å­—ç¬¦ã€‚"), link(title="[Wikipedia]", url="https://en.wikipedia.org/wiki/List_of_Unicode_characters")
    vocabulary_size = max(indices) + 1  # è¿™æ˜¯ä¸€ä¸ªä¸‹ç•Œ @inspect vocabulary_size
    text("é—®é¢˜ 1ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„è¯æ±‡è¡¨ã€‚")
    text("é—®é¢˜ 2ï¼šè®¸å¤šå­—ç¬¦ç›¸å½“ç½•è§ï¼ˆä¾‹å¦‚ ğŸŒï¼‰ï¼Œè¿™æ˜¯è¯æ±‡è¡¨çš„ä½æ•ˆä½¿ç”¨ã€‚")
    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio


def byte_tokenizer():
    text("## åŸºäºå­—èŠ‚çš„ tokenization")

    text("Unicode å­—ç¬¦ä¸²å¯ä»¥è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—ï¼Œå¯ä»¥ç”¨ 0 åˆ° 255 ä¹‹é—´çš„æ•´æ•°è¡¨ç¤ºã€‚")
    text("æœ€å¸¸è§çš„ Unicode ç¼–ç æ˜¯ "), link(title="UTF-8", url="https://en.wikipedia.org/wiki/UTF-8")

    text("ä¸€äº› Unicode å­—ç¬¦ç”±ä¸€ä¸ªå­—èŠ‚è¡¨ç¤ºï¼š")
    assert bytes("a", encoding="utf-8") == b"a"
    text("å…¶ä»–å­—ç¬¦éœ€è¦å¤šä¸ªå­—èŠ‚ï¼š")
    assert bytes("ğŸŒ", encoding="utf-8") == b"\xf0\x9f\x8c\x8d"

    text("ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª `Tokenizer` å¹¶ç¡®ä¿å®ƒå¾€è¿”ï¼š")
    tokenizer = ByteTokenizer()
    string = "Hello, ğŸŒ! ä½ å¥½!"  # @inspect string
    indices = tokenizer.encode(string)  # @inspect indices
    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
    assert string == reconstructed_string

    text("è¯æ±‡è¡¨åˆå¥½åˆå°ï¼šä¸€ä¸ªå­—èŠ‚å¯ä»¥è¡¨ç¤º 256 ä¸ªå€¼ã€‚")
    vocabulary_size = 256  # @inspect vocabulary_size
    text("å‹ç¼©ç‡å¦‚ä½•ï¼Ÿ")
    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio
    assert compression_ratio == 1
    text("å‹ç¼©æ¯”å¾ˆç³Ÿç³•ï¼Œè¿™æ„å‘³ç€åºåˆ—ä¼šå¤ªé•¿ã€‚")
    text("è€ƒè™‘åˆ° Transformer çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯æœ‰é™çš„ï¼ˆå› ä¸º attention æ˜¯äºŒæ¬¡çš„ï¼‰ï¼Œè¿™çœ‹èµ·æ¥ä¸å¤ªå¥½...")


def word_tokenizer():
    text("## åŸºäºå•è¯çš„ tokenization")

    text("å¦ä¸€ç§æ–¹æ³•ï¼ˆæ›´æ¥è¿‘ NLP ä¸­ç»å…¸åšæ³•ï¼‰æ˜¯å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•è¯ã€‚")
    string = "I'll say supercalifragilisticexpialidocious!"

    segments = regex.findall(r"\w+|.", string)  # @inspect segments
    text("è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å°†æ‰€æœ‰å­—æ¯æ•°å­—å­—ç¬¦ä¿æŒåœ¨ä¸€èµ·ï¼ˆå•è¯ï¼‰ã€‚")

    text("è¿™æ˜¯ä¸€ä¸ªæ›´é«˜çº§çš„ç‰ˆæœ¬ï¼š")
    pattern = GPT2_TOKENIZER_REGEX  # @inspect pattern
    segments = regex.findall(pattern, string)  # @inspect segments

    text("è¦å°†å…¶è½¬æ¢ä¸º `Tokenizer`ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›ç‰‡æ®µæ˜ å°„ä¸ºæ•´æ•°ã€‚")
    text("ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä»æ¯ä¸ªç‰‡æ®µåˆ°æ•´æ•°çš„æ˜ å°„ã€‚")

    text("ä½†å­˜åœ¨é—®é¢˜ï¼š")
    text("- å•è¯æ•°é‡å·¨å¤§ï¼ˆå°±åƒ Unicode å­—ç¬¦ä¸€æ ·ï¼‰ã€‚")
    text("- è®¸å¤šå•è¯å¾ˆç½•è§ï¼Œæ¨¡å‹ä¸ä¼šå­¦åˆ°å¤ªå¤šå…³äºå®ƒä»¬çš„ä¸œè¥¿ã€‚")
    text("- è¿™æ˜¾ç„¶ä¸èƒ½æä¾›å›ºå®šçš„è¯æ±‡è¡¨å¤§å°ã€‚")

    text("è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ–°å•è¯ä¼šå¾—åˆ°ä¸€ä¸ªç‰¹æ®Šçš„ UNK tokenï¼Œè¿™å¾ˆä¸‘é™‹ï¼Œå¹¶ä¸”ä¼šæä¹±å›°æƒ‘åº¦è®¡ç®—ã€‚")

    vocabulary_size = "è®­ç»ƒæ•°æ®ä¸­ä¸åŒç‰‡æ®µçš„æ•°é‡"
    compression_ratio = get_compression_ratio(string, segments)  # @inspect compression_ratio


def bpe_tokenizer():
    text("## Byte Pair Encoding (BPE)")
    link(title="[Wikipedia]", url="https://en.wikipedia.org/wiki/Byte_pair_encoding")
    text("BPE ç®—æ³•ç”± Philip Gage äº 1994 å¹´å¼•å…¥ç”¨äºæ•°æ®å‹ç¼©ã€‚"), article_link("http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM")
    text("å®ƒè¢«æ”¹ç¼–ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘çš„ NLPã€‚"), link(sennrich_2016)
    text("ï¼ˆä¹‹å‰ï¼Œè®ºæ–‡ä¸€ç›´åœ¨ä½¿ç”¨åŸºäºå•è¯çš„ tokenizationã€‚ï¼‰")
    text("BPE éšåè¢« GPT-2 ä½¿ç”¨ã€‚"), link(gpt2)

    text("åŸºæœ¬æ€æƒ³ï¼šåœ¨åŸå§‹æ–‡æœ¬ä¸Š*è®­ç»ƒ* tokenizer ä»¥è‡ªåŠ¨ç¡®å®šè¯æ±‡è¡¨ã€‚")
    text("ç›´è§‰ï¼šå¸¸è§çš„å­—ç¬¦åºåˆ—ç”±å•ä¸ª token è¡¨ç¤ºï¼Œç½•è§çš„åºåˆ—ç”±è®¸å¤š token è¡¨ç¤ºã€‚")

    text("GPT-2 è®ºæ–‡ä½¿ç”¨åŸºäºå•è¯çš„ tokenization å°†æ–‡æœ¬åˆ†è§£ä¸ºåˆå§‹ç‰‡æ®µï¼Œå¹¶åœ¨æ¯ä¸ªç‰‡æ®µä¸Šè¿è¡ŒåŸå§‹ BPE ç®—æ³•ã€‚")
    text("è‰å›¾ï¼šä»æ¯ä¸ªå­—èŠ‚ä½œä¸º token å¼€å§‹ï¼Œå¹¶è¿ç»­åˆå¹¶æœ€å¸¸è§çš„ç›¸é‚» token å¯¹ã€‚")

    text("## è®­ç»ƒ tokenizer")
    string = "the cat in the hat"  # @inspect string
    params = train_bpe(string, num_merges=3)

    text("## ä½¿ç”¨ tokenizer")
    text("ç°åœ¨ï¼Œç»™å®šä¸€ä¸ªæ–°æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶è¿›è¡Œç¼–ç ã€‚")
    tokenizer = BPETokenizer(params)
    string = "the quick brown fox"  # @inspect string
    indices = tokenizer.encode(string)  # @inspect indices
    reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
    assert string == reconstructed_string

    text("åœ¨ä½œä¸š 1 ä¸­ï¼Œä½ å°†é€šè¿‡ä»¥ä¸‹æ–¹å¼è¶…è¶Šè¿™ä¸€ç‚¹ï¼š")
    text("- encode() å½“å‰å¾ªç¯éå†æ‰€æœ‰åˆå¹¶ã€‚åªå¾ªç¯é‡è¦çš„åˆå¹¶ã€‚")
    text("- æ£€æµ‹å¹¶ä¿ç•™ç‰¹æ®Š tokenï¼ˆä¾‹å¦‚ <|endoftext|>ï¼‰ã€‚")
    text("- ä½¿ç”¨é¢„ tokenizationï¼ˆä¾‹å¦‚ GPT-2 tokenizer regexï¼‰ã€‚")
    text("- å°è¯•ä½¿å®ç°å°½å¯èƒ½å¿«ã€‚")


def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  # @inspect string, @inspect num_merges
    text("ä» `string` çš„å­—èŠ‚åˆ—è¡¨å¼€å§‹ã€‚")
    indices = list(map(int, string.encode("utf-8")))  # @inspect indices
    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index
    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes

    for i in range(num_merges):
        text("è®¡ç®—æ¯å¯¹ token çš„å‡ºç°æ¬¡æ•°")
        counts = defaultdict(int)
        for index1, index2 in zip(indices, indices[1:]):  # å¯¹äºæ¯ä¸ªç›¸é‚»å¯¹
            counts[(index1, index2)] += 1  # @inspect counts

        text("æ‰¾åˆ°æœ€å¸¸è§çš„å¯¹ã€‚")
        pair = max(counts, key=counts.get)  # @inspect pair
        index1, index2 = pair

        text("åˆå¹¶è¯¥å¯¹ã€‚")
        new_index = 256 + i  # @inspect new_index
        merges[pair] = new_index  # @inspect merges
        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab
        indices = merge(indices, pair, new_index)  # @inspect indices

    return BPETokenizerParams(vocab=vocab, merges=merges)


if __name__ == "__main__":
    main()
