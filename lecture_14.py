from dataclasses import dataclass
import math
import torch
import torch.nn as nn
from torch.nn.functional import softmax
import numpy as np
import kenlm
import fasttext
import itertools
import mmh3
from bitarray import bitarray
from basic_util import count, repeat
from file_util import download_file
from execute_util import text, image, link
from lecture_util import article_link, named_link
from references import dolma

def main():
    text("ä¸ŠèŠ‚è¯¾ï¼šè®­ç»ƒè¯­è¨€æ¨¡å‹æ‰€ç”¨æ•°æ®é›†æ¦‚è§ˆ")
    text("- åœ¨çº¿æœåŠ¡ (GitHub) â†’ è½¬å‚¨/çˆ¬å– (GH Archive) â†’ å¤„ç†åçš„æ•°æ® (The Stack)")
    text("- å¤„ç†æµç¨‹ï¼šHTMLè½¬æ–‡æœ¬ã€è¯­è¨€/è´¨é‡/æœ‰å®³å†…å®¹è¿‡æ»¤ã€å»é‡")

    text("æœ¬èŠ‚è¯¾ï¼šæ·±å…¥æ¢è®¨æŠ€æœ¯ç»†èŠ‚")
    text("- è¿‡æ»¤ç®—æ³•ï¼ˆä¾‹å¦‚ï¼šåˆ†ç±»å™¨ï¼‰")
    text("- è¿‡æ»¤åº”ç”¨ï¼ˆä¾‹å¦‚ï¼šè¯­è¨€è¯†åˆ«ã€è´¨é‡è¿‡æ»¤ã€æœ‰å®³å†…å®¹è¿‡æ»¤ï¼‰")
    text("- å»é‡ï¼ˆä¾‹å¦‚ï¼šBloom filtersã€MinHashã€LSHï¼‰")

    filtering_algorithms()
    filtering_applications()
    deduplication()

    text("### æ€»ç»“")
    text("- ç®—æ³•å·¥å…·ï¼šn-gram æ¨¡å‹ (KenLM)ã€åˆ†ç±»å™¨ (fastText)ã€é‡è¦æ€§é‡é‡‡æ · (DSIR)")
    text("- åº”ç”¨åœºæ™¯ï¼šè¯­è¨€è¯†åˆ«ã€è´¨é‡è¿‡æ»¤ã€æœ‰å®³å†…å®¹è¿‡æ»¤")
    text("- å»é‡ï¼šhashing å¯æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œæ¨¡ç³ŠåŒ¹é…")
    text("- ç°åœ¨ä½ å·²ç»æŒæ¡äº†å·¥å…·ï¼ˆæŠ€æœ¯ï¼‰ï¼Œåªéœ€è¦èŠ±æ—¶é—´å¤„ç†æ•°æ®ï¼ˆç§¯ç´¯ç›´è§‰ï¼‰")


def filtering_algorithms():
    text("ç®—æ³•æ„å»ºæ¨¡å—ï¼š")
    text("- ç»™å®šä¸€äº›**ç›®æ ‡æ•°æ®** T å’Œå¤§é‡**åŸå§‹æ•°æ®** Rï¼Œä» R ä¸­æ‰¾åˆ°ä¸ T ç›¸ä¼¼çš„å­é›† T'ã€‚")
    image("images/raw-target-schema.png", width=600)

    text("è¿‡æ»¤ç®—æ³•çš„ç†æƒ³ç‰¹æ€§ï¼š")
    text("- ä»ç›®æ ‡æ•°æ®ä¸­æ³›åŒ–ï¼ˆå¸Œæœ› T å’Œ T' æ˜¯ä¸åŒçš„ï¼‰")
    text("- æå¿«çš„é€Ÿåº¦ï¼ˆå¿…é¡»åœ¨ R ä¸Šè¿è¡Œï¼Œè€Œ R éå¸¸åºå¤§ï¼‰")

    kenlm_main()         # è®­ç»ƒ n-gram æ¨¡å‹
    fasttext_main()      # è®­ç»ƒåˆ†ç±»å™¨
    dsir_main()          # è®­ç»ƒ bag of n-grams æ¨¡å‹ï¼Œè¿›è¡Œé‡è¦æ€§é‡é‡‡æ ·
    filtering_summary()

    text("æ•°æ®é€‰æ‹©ç»¼è¿°è®ºæ–‡ "), link("https://arxiv.org/abs/2402.16827")


def kenlm_main():
    text("**n-gram æ¨¡å‹ä¸ Kneser-Ney å¹³æ»‘** "), article_link("https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing")
    text("- KenLMï¼šæœ€åˆä¸ºæœºå™¨ç¿»è¯‘å¼€å‘çš„å¿«é€Ÿå®ç° "), named_link("code", "https://kheafield.com/code/kenlm/")
    text("- ç”¨äºæ•°æ®è¿‡æ»¤çš„å¸¸è§è¯­è¨€æ¨¡å‹")
    text("- æå…¶ç®€å•/å¿«é€Ÿ - åªéœ€è®¡æ•°å’Œå½’ä¸€åŒ–")

    text("### æ¦‚å¿µ")
    text("n-gram è¯­è¨€æ¨¡å‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼š")
    text("- n = 3: p(in | the cat) = count(the cat in) / count(the cat)")
    text("é—®é¢˜ï¼šç¨€ç–è®¡æ•°ï¼ˆå¯¹äºå¤§çš„ nï¼Œè®¸å¤š n-gram çš„è®¡æ•°ä¸º 0ï¼‰")
    text("è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Kneser-Ney å¹³æ»‘å¤„ç†æœªè§è¿‡çš„ n-gram "), article_link("https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing")
    text("- p(in | the cat) ä¹Ÿä¾èµ–äº p(in | cat)")

    # ä¸‹è½½ KenLM è¯­è¨€æ¨¡å‹
    model_url = "https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.arpa.bin"
    model_path = "var/en.arpa.bin"
    download_file(model_url, model_path)
    model = kenlm.Model(model_path)

    # ä½¿ç”¨è¯­è¨€æ¨¡å‹
    def compute(content: str):
        # ç®€å•çš„é¢„å¤„ç†
        content = "<s> " + content.replace(",", " ,").replace(".", " .") + " </s>"

        # log p(content)
        score = model.score(content)

        # Perplexity é€šè¿‡ token æ•°é‡å½’ä¸€åŒ–ï¼Œé¿å…åå‘çŸ­æ–‡æ¡£
        num_tokens = len(list(model.full_scores(content)))
        perplexity = math.exp(-score / num_tokens)

        return score, perplexity

    score, perplexity = compute("Stanford University was founded in 1885 by Leland and Jane Stanford as a tribute to the memory of their only child, Leland Stanford Jr.")  # @inspect score, @inspect perplexity
    score, perplexity = compute("If you believe that the course staff made an objective error in grading, you may submit a regrade request on Gradescope within 3 days after the grades are released.")  # @inspect score, @inspect perplexity
    score, perplexity = compute("asdf asdf asdf asdf asdf")  # @inspect score, @inspect perplexity
    score, perplexity = compute("the the the the the the the the the the the the the the the the")  # @inspect score, @inspect perplexity

    text("### CCNet")
    link("https://arxiv.org/pdf/1911.00359")
    text("- é¡¹ç›®æ˜¯æ–‡æœ¬æ®µè½")
    text("- æŒ‰ perplexity é€’å¢æ’åºæ®µè½")
    text("- ä¿ç•™å‰ 1/3")
    text("- åœ¨ LLaMA ä¸­ä½¿ç”¨è¿‡")

    text("æ€»ç»“ï¼šKneser-Ney n-gram è¯­è¨€æ¨¡å‹ï¼ˆKenLM å®ç°ï¼‰å¿«é€Ÿä½†ç²—ç³™")


def fasttext_main():
    text("fastText åˆ†ç±»å™¨ "), link("https://arxiv.org/pdf/1607.01759")
    text("- ä»»åŠ¡ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆä¾‹å¦‚ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼‰")
    text("- ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªå¿«é€Ÿçš„æ–‡æœ¬åˆ†ç±»å™¨")
    text("- ä»–ä»¬å‘ç°å®ƒå’Œæ…¢å¾—å¤šçš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ä¸€æ ·å¥½")

    text("### åŸºçº¿ï¼šbag of wordsï¼ˆä¸æ˜¯ä»–ä»¬åšçš„ï¼‰")
    L = 32                              # è¾“å…¥é•¿åº¦
    V = 8192                            # è¯æ±‡è¡¨å¤§å°
    K = 64                              # ç±»åˆ«æ•°é‡
    W = nn.Embedding(V, K)              # Embedding å‚æ•° (V x K)
    x = torch.randint(V, (L,))          # è¾“å…¥ tokens (L) - ä¾‹å¦‚ï¼š["the", "cat", "in", "the", "hat"]
    y = softmax(W(x).mean(dim=0))       # è¾“å‡ºæ¦‚ç‡ (K)
    text("é—®é¢˜ï¼šV*K ä¸ªå‚æ•°ï¼ˆå¯èƒ½éå¸¸å¤§ï¼‰")

    text("### fastText åˆ†ç±»å™¨ï¼šbag of word embeddings")
    H = 16                              # éšè—ç»´åº¦
    W = nn.Embedding(V, H)              # Embedding å‚æ•° (V x H)
    U = nn.Linear(H, K)                 # Head å‚æ•° (H x K)
    y = softmax(U(W(x).mean(dim=0)))    # è¾“å‡ºæ¦‚ç‡ (K)
    text("åªæœ‰ H*(V + K) ä¸ªå‚æ•°")

    text("å®ç°ï¼š")
    text("- å¹¶è¡ŒåŒ–ã€å¼‚æ­¥ SGD")
    text("- å­¦ä¹ ç‡ï¼šä» [æŸä¸ªæ•°å€¼] åˆ° 0 çš„çº¿æ€§æ’å€¼ "), article_link("https://github.com/facebookresearch/fastText/blob/main/src/fasttext.cc#L653")

    text("### Bag of n-grams")
    x = ["the cat", "cat in", "in the", "the hat"]  # @inspect x
    text("é—®é¢˜ï¼šbigram çš„æ•°é‡å¯èƒ½ä¼šå¾ˆå¤§ï¼ˆè€Œä¸”å¯èƒ½æ˜¯æ— ç•Œçš„ï¼‰")
    text("è§£å†³æ–¹æ¡ˆï¼šhashing trick")
    num_bins = 8  # å®é™…ä¸­ï¼Œä½¿ç”¨ 10M ä¸ª bins
    hashed_x = [mmh3.hash(bigram) % num_bins for bigram in x]  # @inspect hashed_x

    text("- å¯¹äºè´¨é‡è¿‡æ»¤ï¼Œæˆ‘ä»¬æœ‰ K = 2 ä¸ªç±»åˆ«ï¼ˆå¥½ vs åï¼‰")
    text("- åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒfastText åªæ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼ˆH = K = 2ï¼‰")

    text("ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•åˆ†ç±»å™¨ï¼ˆä¾‹å¦‚ï¼šBERTã€Llamaï¼‰ï¼Œåªæ˜¯ä¼šæ›´æ…¢")


def dsir_main():
    text("é€šè¿‡é‡è¦æ€§é‡é‡‡æ ·è¿›è¡Œè¯­è¨€æ¨¡å‹æ•°æ®é€‰æ‹© (DSIR) "), link("https://arxiv.org/abs/2302.03169")
    image("https://www.jinghong-chen.net/content/images/size/w1200/2023/12/Screenshot-2023-12-24-at-17.41.38.png", width=600)

    importance_sampling()

    text("è®¾ç½®ï¼š")
    text("- ç›®æ ‡æ•°æ®é›† D_pï¼ˆå°ï¼‰")
    text("- æè®®ï¼ˆåŸå§‹ï¼‰æ•°æ®é›† D_qï¼ˆå¤§ï¼‰")

    text("æ–¹æ³• 1ï¼š")
    text("- å°†ç›®æ ‡åˆ†å¸ƒ p æ‹Ÿåˆåˆ° D_p")
    text("- å°†æè®®åˆ†å¸ƒ q æ‹Ÿåˆåˆ° D_q")
    text("- ä½¿ç”¨ pã€q å’ŒåŸå§‹æ ·æœ¬ D_q è¿›è¡Œé‡è¦æ€§é‡é‡‡æ ·")
    text("é—®é¢˜ï¼šç›®æ ‡æ•°æ® D_p å¤ªå°ï¼Œæ— æ³•ä¼°è®¡ä¸€ä¸ªå¥½çš„æ¨¡å‹")

    text("æ–¹æ³• 2ï¼šä½¿ç”¨ hashed n-grams")
    training_text = "the cat in the hat"

    # å¯¹ n-grams è¿›è¡Œ hash
    num_bins = 4
    def get_hashed_ngrams(text: str):
        ngrams = text.split(" ")  # ç›®å‰ä½¿ç”¨ Unigram
        return [mmh3.hash(ngram) % num_bins for ngram in ngrams]

    training_hashed_ngrams = get_hashed_ngrams(training_text)  # @inspect training_hashed_ngrams

    # å­¦ä¹  unigram æ¨¡å‹
    probs = [count(training_hashed_ngrams, x) / len(training_hashed_ngrams) for x in range(num_bins)]  # @inspect probs

    # è¯„ä¼°ä»»æ„å¥å­çš„æ¦‚ç‡
    hashed_ngrams = get_hashed_ngrams("the text")  # @inspect hashed_ngrams
    prob = np.prod([probs[x] for x in hashed_ngrams])  # @inspect prob
    text("ç»“æœï¼šDSIR åœ¨ [GLUE](https://gluebenchmark.com/) benchmark ä¸Šç•¥ä¼˜äºå¯å‘å¼åˆ†ç±»ï¼ˆfastTextï¼‰")
    image("images/dsir-results.png", width=700)
    
    text("ä¸ fastText çš„æ¯”è¾ƒï¼š")
    text("- å»ºæ¨¡åˆ†å¸ƒæ˜¯ä¸€ç§æ›´æœ‰åŸåˆ™çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰å¤šæ ·æ€§")
    text("- è®¡ç®—å¤æ‚åº¦ç›¸ä¼¼")
    text("- ä¸¤è€…éƒ½å¯ä»¥é€šè¿‡æ›´å¥½çš„å»ºæ¨¡æ¥æ”¹è¿›")


def importance_sampling():
    text("è®¾ç½®ï¼š")
    text("- ç›®æ ‡åˆ†å¸ƒ pï¼ˆæƒ³è¦ä»è¿™é‡Œé‡‡æ ·ï¼‰")
    text("- æè®®åˆ†å¸ƒ qï¼ˆå·²æœ‰ä»è¿™é‡Œçš„æ ·æœ¬ï¼‰")

    vocabulary = [0, 1, 2, 3]
    p = [0.1, 0.2, 0.3, 0.4]
    q = [0.4, 0.3, 0.2, 0.1]

    # 1. ä» q é‡‡æ ·
    n = 100
    samples = np.random.choice(vocabulary, p=q, size = n)  # @inspect samples
    text(f"æ ·æœ¬ (q): {samples}")

    # 2. è®¡ç®—æ ·æœ¬çš„æƒé‡ (w \propto p/q)
    w = [p[x] / q[x] for x in samples]  # @inspect w
    z = sum(w)  # @inspect z
    w = [w_i / z for w_i in w]  # @inspect w

    # 3. é‡é‡‡æ ·
    samples = np.random.choice(samples, p=w, size=n)  # @inspect samples
    text(f"é‡é‡‡æ · (p): {samples}")


def filtering_summary():
    text("å®ç°ï¼šKenLMã€fastTextã€DSIR")

    text("### é€šç”¨æ¡†æ¶")
    text("ç»™å®šç›®æ ‡ T å’ŒåŸå§‹æ•°æ® Rï¼Œæ‰¾åˆ° R ä¸­ä¸ T ç›¸ä¼¼çš„å­é›†")
    text("1. åŸºäº R å’Œ T ä¼°è®¡æŸä¸ªæ¨¡å‹å¹¶æ¨å¯¼è¯„åˆ†å‡½æ•°")
    text("2. æ ¹æ®è¯„åˆ†ä¿ç•™ R ä¸­çš„æ ·æœ¬")

    text("### æ¡†æ¶çš„å®ä¾‹åŒ–")

    text("T çš„ç”Ÿæˆæ¨¡å‹ (KenLM)ï¼š")
    text("1. score(x) = p_T(x)")
    text("2. ä¿ç•™ score(x) >= threshold çš„æ ·æœ¬ xï¼ˆéšæœºåœ°ï¼‰")

    text("åˆ¤åˆ«åˆ†ç±»å™¨ (fastText)ï¼š")
    text("1. score(x) = p(T | x)")
    text("2. ä¿ç•™ score(x) >= threshold çš„æ ·æœ¬ xï¼ˆéšæœºåœ°ï¼‰")

    text("é‡è¦æ€§é‡é‡‡æ · (DSIR)ï¼š")
    text("1. score(x) = p_T(x) / p_R(x)")
    text("2. ä»¥ä¸ score(x) æˆæ­£æ¯”çš„æ¦‚ç‡é‡é‡‡æ ·æ ·æœ¬ x")


def filtering_applications():
    text("ç›¸åŒçš„æ•°æ®è¿‡æ»¤æœºåˆ¶å¯ç”¨äºä¸åŒçš„è¿‡æ»¤ä»»åŠ¡ã€‚")
    language_identification()
    quality_filtering()
    toxicity_filtering()


def language_identification():
    text("è¯­è¨€è¯†åˆ«ï¼šæ‰¾åˆ°ç‰¹å®šè¯­è¨€çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼šè‹±è¯­ï¼‰")

    text("ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨å¤šè¯­è¨€ï¼Ÿ")
    text("- æ•°æ®ï¼šéš¾ä»¥å¯¹ä»»ä½•ç»™å®šè¯­è¨€è¿›è¡Œé«˜è´¨é‡æ•°æ®çš„ç­–åˆ’/å¤„ç†")
    text("- è®¡ç®—ï¼šåœ¨è®¡ç®—å—é™çš„æƒ…å†µä¸‹ï¼Œåˆ†é…ç»™ä»»ä½•ç»™å®šè¯­è¨€çš„è®¡ç®—/tokens æ›´å°‘")
    text("æ¨¡å‹åœ¨å¤šè¯­è¨€æ€§ä¸Šçš„å·®å¼‚ï¼š")
    text("- è‹±è¯­åœ¨ BLOOM ä¸­åªå  30%ï¼ˆè®­ç»ƒä¸è¶³ï¼‰ï¼Œè‹±è¯­æ€§èƒ½å—æŸ "), link("https://arxiv.org/pdf/2303.03915")
    text("- å¤§å¤šæ•°å‰æ²¿æ¨¡å‹ï¼ˆGPT-4ã€Claudeã€Geminiã€Llamaã€Qwenï¼‰éƒ½æ˜¯é«˜åº¦å¤šè¯­è¨€çš„ï¼ˆè®­ç»ƒå……åˆ†ï¼‰")

    text("fastText è¯­è¨€è¯†åˆ« "), article_link("https://fasttext.cc/docs/en/language-identification.html")
    text("- å¼€ç®±å³ç”¨çš„åˆ†ç±»å™¨")
    text("- æ”¯æŒ 176 ç§è¯­è¨€")
    text("- åœ¨å¤šè¯­è¨€ç½‘ç«™ä¸Šè®­ç»ƒï¼šWikipediaã€Tatoebaï¼ˆç¿»è¯‘ç½‘ç«™ï¼‰å’Œ SETimesï¼ˆä¸œå—æ¬§æ–°é—»ï¼‰")

    text("ç¤ºä¾‹ï¼šDolma ä¿ç•™ p(English) >= 0.5 çš„é¡µé¢ "), link(dolma)
    
    # ä¸‹è½½æ¨¡å‹
    model_url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
    model_path = "var/lid.176.bin"
    download_file(model_url, model_path)
    model = fasttext.load_model(model_path)

    # è¿›è¡Œé¢„æµ‹
    predictions = model.predict(["The quick brown fox jumps over the lazy dog."])  # è‹±è¯­ @inspect predictions
    predictions = model.predict(["The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog."])  # é‡å¤ @inspect predictions
    predictions = model.predict(["OMG that movie was ğŸ”¥ğŸ”¥! So dope ğŸ˜ğŸ¤˜!"])  # éæ­£å¼è‹±è¯­ @inspect predictions
    predictions = model.predict(["Auf dem Wasser zu singen"])  # å¾·è¯­ @inspect predictions
    predictions = model.predict(["The quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$."])  # Latex @inspect predictions
    predictions = model.predict(["for (int i = 0; i < 10; i++)"])  # C++ @inspect predictions
    predictions = model.predict(["Hello!"])  # è‹±è¯­ @inspect predictions
    predictions = model.predict(["Bonjour!"])  # æ³•è¯­ @inspect predictions
    predictions = model.predict(["Feliz Navidad / PrÃ³spero aÃ±o y felicidad / I wanna wish you a Merry Christmas"])  # è¥¿ç­ç‰™è¯­ + è‹±è¯­ @inspect predictions

    text("æ³¨æ„äº‹é¡¹ï¼š")
    text("- å¯¹äºçŸ­åºåˆ—å¾ˆå›°éš¾")
    text("- å¯¹äºä½èµ„æºè¯­è¨€å¾ˆå›°éš¾")
    text("- å¯èƒ½ä¼šæ„å¤–è¿‡æ»¤æ‰è‹±è¯­æ–¹è¨€")
    text("- å¯¹äºç›¸ä¼¼è¯­è¨€å¾ˆå›°éš¾ï¼ˆé©¬æ¥è¯­å’Œå°å°¼è¯­ï¼‰")
    text("- å¯¹äºè¯­ç è½¬æ¢å®šä¹‰ä¸æ¸…ï¼ˆä¾‹å¦‚ï¼šè¥¿ç­ç‰™è¯­ + è‹±è¯­ï¼‰")

    text("OpenMathText "), link("https://arxiv.org/pdf/2310.06786")
    text("- ç›®æ ‡ï¼šä» CommonCrawl ä¸­ç­–åˆ’å¤§å‹æ•°å­¦æ–‡æœ¬è¯­æ–™åº“")
    text("- ä½¿ç”¨è§„åˆ™è¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼šåŒ…å« latex å‘½ä»¤ï¼‰")
    text("- åœ¨ ProofPile ä¸Šè®­ç»ƒ KenLMï¼Œå¦‚æœ perplexity < 15000 åˆ™ä¿ç•™")
    text("- è®­ç»ƒ fastText åˆ†ç±»å™¨é¢„æµ‹æ•°å­¦å†™ä½œï¼Œé˜ˆå€¼ä¸º 0.17ï¼ˆå¦‚æœæ˜¯æ•°å­¦ï¼‰ï¼Œ0.8ï¼ˆå¦‚æœä¸æ˜¯æ•°å­¦ï¼‰")
    text("ç»“æœï¼šäº§ç”Ÿäº† 14.7B tokensï¼Œç”¨äºè®­ç»ƒ 1.4B æ¨¡å‹ï¼Œæ•ˆæœä¼˜äºåœ¨ 20 å€æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹")


def quality_filtering():
    text("- æœ‰äº›æ•…æ„ä¸ä½¿ç”¨åŸºäºæ¨¡å‹çš„è¿‡æ»¤ï¼ˆC4ã€Gopherã€RefinedWebã€FineWebã€Dolmaï¼‰")
    text("- æœ‰äº›ä½¿ç”¨åŸºäºæ¨¡å‹çš„è¿‡æ»¤ï¼ˆGPT-3ã€LLaMAã€DCLMï¼‰[æ­£åœ¨æˆä¸ºå¸¸æ€]")

    text("**GPT-3** "), link("https://arxiv.org/pdf/2005.14165")  # Appendix A
    text("- æ­£æ ·æœ¬ï¼šæ¥è‡ª {Wikipediaã€WebText2ã€Books1ã€Books2} çš„æ ·æœ¬")
    text("- è´Ÿæ ·æœ¬ï¼šæ¥è‡ª CommonCrawl çš„æ ·æœ¬")
    image("https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Probability_density_function_of_Pareto_distribution.svg/325px-Probability_density_function_of_Pareto_distribution.svg.png", width=0.5)
    text("åŸºäºè¯ç‰¹å¾è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ "), article_link("https://spark.apache.org/docs/latest/ml-features#tokenizer")
    text("æ ¹æ®è¯„åˆ†éšæœºä¿ç•™æ–‡æ¡£")
    def keep_document(score: float) -> bool:
        return np.random.pareto(9) > 1 - score

    text("** LLaMA/RedPajama** "), link("https://arxiv.org/pdf/2302.13971")
    text("- æ­£æ ·æœ¬ï¼šæ¥è‡ª Wikipedia **å¼•ç”¨**çš„é¡µé¢çš„æ ·æœ¬")
    text("- è´Ÿæ ·æœ¬ï¼šæ¥è‡ª CommonCrawl çš„æ ·æœ¬")
    text("- ä¿ç•™è¢«åˆ†ç±»ä¸ºæ­£çš„æ–‡æ¡£")

    text("**phi-1** "), link("https://arxiv.org/pdf/2306.11644")
    text("ç†å¿µï¼šä½¿ç”¨çœŸæ­£é«˜è´¨é‡çš„æ•°æ®ï¼ˆæ•™ç§‘ä¹¦ï¼‰è®­ç»ƒå°æ¨¡å‹ï¼ˆ1.5Bï¼‰")
    text("åŒ…æ‹¬æ¥è‡ª GPT 3.5ï¼ˆåæ¥ï¼šGPT-4ï¼‰çš„åˆæˆæ•°æ®å’Œè¿‡æ»¤æ•°æ®")

    R = "Python subset of the Stack"   # åŸå§‹æ•°æ®
    prompt = "determine its educational value for a student whose goal is to learn basic coding concepts"
    T = "Use GPT-4 with this prompt to classify 100K subset of R to get positive examples"
    text("ä½¿ç”¨é¢„è®­ç»ƒ codegen æ¨¡å‹çš„è¾“å‡º embedding åœ¨ T ä¸Šè®­ç»ƒ random forest åˆ†ç±»å™¨")
    text("ä» R ä¸­é€‰æ‹©è¢«åˆ†ç±»å™¨åˆ†ç±»ä¸ºæ­£çš„æ•°æ®")

    text("åœ¨ [HumanEval](https://huggingface.co/datasets/openai_humaneval) ä¸Šçš„ç»“æœï¼š")
    text("- åœ¨ The Stack çš„ Python å­é›†ä¸Šè®­ç»ƒ 1.3B LMï¼ˆæ€§èƒ½ï¼š96K æ­¥å 12.19%ï¼‰")
    text("- åœ¨æ–°çš„è¿‡æ»¤å­é›†ä¸Šè®­ç»ƒ 1.3B LMï¼ˆæ€§èƒ½ï¼š36K æ­¥å 17.68%ï¼‰- æ›´å¥½ï¼")


@dataclass
class Example:
    text: str
    label: int


def toxicity_filtering():
    # è­¦å‘Šï¼šä»¥ä¸‹å¯èƒ½åŒ…å«å†’çŠ¯æ€§å†…å®¹
    text("Dolma ä¸­çš„æœ‰å®³å†…å®¹è¿‡æ»¤ "), link(dolma)
    
    text("æ•°æ®é›†ï¼šJigsaw Toxic Comments æ•°æ®é›†ï¼ˆ2018ï¼‰"), named_link("dataset", "https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge")
    text("- é¡¹ç›®ç›®æ ‡ï¼šå¸®åŠ©äººä»¬åœ¨çº¿ä¸Šè¿›è¡Œæ›´å¥½çš„è®¨è®º "), article_link("https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/discussion/46064")
    text("- æ•°æ®ï¼šWikipedia è®¨è®ºé¡µä¸Šçš„è¯„è®ºï¼Œæ ‡æ³¨ä¸º {toxic, severe_toxic, obscene, threat, insult, identity_hate}")

    text("è®­ç»ƒäº† 2 ä¸ª fastText åˆ†ç±»å™¨")
    text("- hateï¼šæ­£æ ·æœ¬ = {unlabeled, obscene}ï¼Œè´Ÿæ ·æœ¬ = å…¶ä»–æ‰€æœ‰")
    text("- NSFWï¼šæ­£æ ·æœ¬ = {obscene}ï¼Œè´Ÿæ ·æœ¬ = å…¶ä»–æ‰€æœ‰")

    # æ•°æ®é›†ä¸­çš„ç¤ºä¾‹ï¼š(obscene, text)
    train_examples = [
        Example(label=0, text="Are you threatening me for disputing neutrality? I know in your country it's quite common to bully your way through a discussion and push outcomes you want. But this is not Russia."),
        Example(label=1, text="Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!"),
    ]

    # ä¸‹è½½æ¨¡å‹
    model_url = "https://dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_nsfw_final.bin"
    model_path = "var/jigsaw_fasttext_bigrams_nsfw_final.bin"
    download_file(model_url, model_path)
    model = fasttext.load_model(model_path)

    # è¿›è¡Œé¢„æµ‹
    predictions = model.predict([train_examples[0].text])  # @inspect predictions
    predictions = model.predict([train_examples[1].text])  # @inspect predictions
    predictions = model.predict(["I love strawberries"])  # @inspect predictions
    predictions = model.predict(["I hate strawberries"])  # @inspect predictions


def print_predict(model, content):
    """åœ¨ `content` ä¸Šè¿è¡Œåˆ†ç±»å™¨ `model` å¹¶æ‰“å°ç»“æœã€‚"""
    predictions = model.predict([content])
    print(predictions)
    #labels, prob =
    #labels = ", ".join(labels)
    #text(f"{content} => {labels} {prob}")


def deduplication():
    text("ä¸¤ç§ç±»å‹çš„é‡å¤ï¼š")
    text("- å®Œå…¨é‡å¤ï¼ˆé•œåƒç«™ç‚¹ã€GitHub forksï¼‰"), named_link("Gutenberg mirrors", "https://www.gutenberg.org/MIRRORS.ALL")
    text("- è¿‘ä¼¼é‡å¤ï¼šç›¸åŒæ–‡æœ¬ä½†æœ‰å‡ ä¸ª token çš„å·®å¼‚")

    text("è¿‘ä¼¼é‡å¤çš„ç¤ºä¾‹ï¼š")
    text("- æœåŠ¡æ¡æ¬¾å’Œè®¸å¯è¯ "), named_link("MIT license", "https://opensource.org/license/mit")
    text("- å…¬å¼åŒ–å†™ä½œï¼ˆå¤åˆ¶/ç²˜è´´æˆ–ä»æ¨¡æ¿ç”Ÿæˆï¼‰"), image("https://d3i71xaburhd42.cloudfront.net/4566c0d22ebf3c31180066ab23b6c445aeec78d5/5-Table1-1.png", width=600)
    text("- å¤åˆ¶/ç²˜è´´ä¸­çš„ç»†å¾®æ ¼å¼å·®å¼‚")

    text("äº§å“æè¿°åœ¨ C4 ä¸­é‡å¤äº† 61,036 æ¬¡")
    text("'\"by combining fantastic ideas, interesting arrangements, and follow the current trends in the field of that make you more inspired and give artistic touches. We'd be honored if you can apply some or all of these design in your wedding.  believe me, brilliant ideas would be perfect if it can be applied in real and make the people around you amazed!")
    named_link("ç¤ºä¾‹é¡µé¢", "https://www.amazon.co.uk/suryagede-100-Graffiti-Gas-Mask/dp/B07CRHT3RG")

    text("å»é‡è®­ç»ƒæ•°æ®ä½¿è¯­è¨€æ¨¡å‹æ›´å¥½ "), link("https://arxiv.org/pdf/2107.06499")
    text("- è®­ç»ƒæ›´é«˜æ•ˆï¼ˆå› ä¸º token æ›´å°‘ï¼‰")
    text("- é¿å…è®°å¿†ï¼ˆå¯ä»¥ç¼“è§£ç‰ˆæƒã€éšç§é—®é¢˜ï¼‰")

    text("è®¾è®¡ç©ºé—´ï¼š")
    text("1. ä»€ä¹ˆæ˜¯é¡¹ç›®ï¼ˆå¥å­ã€æ®µè½ã€æ–‡æ¡£ï¼‰ï¼Ÿ")
    text("2. å¦‚ä½•åŒ¹é…ï¼ˆç²¾ç¡®åŒ¹é…ã€å­˜åœ¨å…±åŒå­é¡¹ã€å…±åŒå­é¡¹çš„æ¯”ä¾‹ï¼‰ï¼Ÿ")
    text("3. é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼ˆåˆ é™¤å…¨éƒ¨ã€åˆ é™¤é™¤ä¸€ä¸ªå¤–çš„æ‰€æœ‰ï¼‰ï¼Ÿ")

    text("å…³é”®æŒ‘æˆ˜ï¼š")
    text("- å»é‡æœ¬è´¨ä¸Šæ˜¯å°†é¡¹ç›®ä¸å…¶ä»–é¡¹ç›®è¿›è¡Œæ¯”è¾ƒ")
    text("- éœ€è¦çº¿æ€§æ—¶é—´ç®—æ³•æ¥æ‰©å±•")

    hash_functions()

    exact_deduplication()
    bloom_filter()

    jaccard_minhash()
    locality_sensitive_hashing()


def hash_functions():
    text("- Hash å‡½æ•° h å°†é¡¹ç›®æ˜ å°„åˆ° hash å€¼ï¼ˆæ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼‰")
    text("- Hash å€¼æ¯”é¡¹ç›®å°å¾—å¤š")
    text("- Hash ç¢°æ’ï¼šh(x) = h(y) å¯¹äº x â‰  y")

    text("æ•ˆç‡å’Œç¢°æ’æŠµæŠ—ä¹‹é—´çš„æƒè¡¡ "),  article_link("https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed")
    text("- å¯†ç å­¦ hash å‡½æ•°ï¼ˆSHA-256ï¼‰ï¼šæŠ—ç¢°æ’ï¼Œæ…¢ï¼ˆç”¨äºæ¯”ç‰¹å¸ï¼‰")
    text("- DJB2ã€MurmurHashã€CityHashï¼šä¸æŠ—ç¢°æ’ï¼Œå¿«ï¼ˆç”¨äº hash è¡¨ï¼‰")

    text("æˆ‘ä»¬å°†ä½¿ç”¨ MurmurHashï¼š")
    h = mmh3.hash("hello")  # @inspect h


def exact_deduplication():
    text("**ç®€å•ç¤ºä¾‹**")
    text("1. é¡¹ç›®ï¼šå­—ç¬¦ä¸²")
    text("2. å¦‚ä½•åŒ¹é…ï¼šç²¾ç¡®åŒ¹é…")
    text("3. è¡ŒåŠ¨ï¼šåˆ é™¤é™¤ä¸€ä¸ªå¤–çš„æ‰€æœ‰")

    # åŸå§‹é¡¹ç›®
    items = ["Hello!", "hello", "hello there", "hello", "hi", "bye"]  # @inspect items

    # è®¡ç®— hash -> å…·æœ‰è¯¥ hash çš„é¡¹ç›®åˆ—è¡¨
    hash_items = itertools.groupby(sorted(items, key=mmh3.hash), key=mmh3.hash)

    # ä»æ¯ç»„ä¸­ä¿ç•™ä¸€ä¸ªé¡¹ç›®
    deduped_items = [next(group) for h, group in hash_items]  # @inspect deduped_items

    text("- ä¼˜ç‚¹ï¼šç®€å•ã€è¯­ä¹‰æ¸…æ™°ã€é«˜ç²¾åº¦")
    text("- ç¼ºç‚¹ï¼šä¸èƒ½å»é‡è¿‘ä¼¼é‡å¤")
    text("- è¿™æ®µä»£ç ä»¥ MapReduce æ–¹å¼ç¼–å†™ï¼Œå¯ä»¥è½»æ¾å¹¶è¡ŒåŒ–å’Œæ‰©å±•")

    text("**C4** "), link("https://arxiv.org/pdf/1910.10683v4")
    text("1. é¡¹ç›®ï¼š3 å¥è¯çš„è·¨åº¦")
    text("2. å¦‚ä½•åŒ¹é…ï¼šä½¿ç”¨ç²¾ç¡®åŒ¹é…")
    text("3. è¡ŒåŠ¨ï¼šåˆ é™¤é™¤ä¸€ä¸ªå¤–çš„æ‰€æœ‰")
    text("è­¦å‘Šï¼šå½“ä»æ–‡æ¡£ä¸­é—´åˆ é™¤ 3 å¥è¯çš„è·¨åº¦æ—¶ï¼Œç”Ÿæˆçš„æ–‡æ¡£å¯èƒ½ä¸è¿è´¯")


def bloom_filter():
    text("ç›®æ ‡ï¼šç”¨äºæµ‹è¯•é›†åˆæˆå‘˜èµ„æ ¼çš„é«˜æ•ˆã€è¿‘ä¼¼æ•°æ®ç»“æ„")

    text("Bloom filter çš„ç‰¹æ€§")
    text("- å†…å­˜é«˜æ•ˆ")
    text("- å¯ä»¥æ›´æ–°ï¼Œä½†ä¸èƒ½åˆ é™¤")
    text("- å¦‚æœè¿”å› 'no'ï¼Œè‚¯å®šæ˜¯ 'no'")
    text("- å¦‚æœè¿”å› 'yes'ï¼Œå¾ˆå¯èƒ½æ˜¯ 'yes'ï¼Œä½†æœ‰å°æ¦‚ç‡æ˜¯ 'no'")
    text("- å¯ä»¥é€šè¿‡æ›´å¤šæ—¶é—´/è®¡ç®—å°†å‡é˜³æ€§ç‡æŒ‡æ•°çº§é™ä½")

    items = ["the", "cat", "in", "the", "hat"]
    non_items = ["what", "who", "why", "when", "where", "which", "how"]

    text("é¦–å…ˆï¼Œä½¿ hash å‡½æ•°çš„èŒƒå›´å˜å°ï¼ˆbins æ•°é‡å°‘ï¼‰ã€‚")
    m = 8  # bins æ•°é‡
    table = build_table(items, m)
    for item in items:
        assert query_table(table, item, m) == 1
    result = {item: query_table(table, item, m) for item in non_items}  # @inspect result
    num_mistakes = count(result.values(), True)  # @inspect num_mistakes
    false_positive_rate = num_mistakes / (len(items) + num_mistakes)  # @inspect false_positive_rate
    text("é—®é¢˜ï¼šå° bins çš„å‡é˜³æ€§")

    text("æœ´ç´ è§£å†³æ–¹æ¡ˆï¼šå¢åŠ  bins çš„æ•°é‡")
    text("é”™è¯¯æ¦‚ç‡æ˜¯ O(1/num_bins)ï¼Œéšå†…å­˜å¤šé¡¹å¼é€’å‡")

    text("æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å¤š hash å‡½æ•°")
    k = 2  # hash å‡½æ•°æ•°é‡
    table = build_table_k(items, m, k)
    for item in items:
        assert query_table_k(table, item, m, k) == 1
    result = {item: query_table_k(table, item, m, k) for item in non_items}  # @inspect result
    num_mistakes = count(result.values(), 1)  # @inspect num_mistakes
    false_positive_rate = num_mistakes / (len(items) + num_mistakes)  # @inspect false_positive_rate
    text("é™ä½äº†å‡é˜³æ€§ç‡ï¼")

    false_positive_rate_analysis()


def false_positive_rate_analysis():
    text("å‡è®¾ hash å‡½æ•°å’Œé¡¹ç›®çš„ç‹¬ç«‹æ€§ "), article_link("https://en.wikipedia.org/wiki/Bloom_filter")
    m = 1000   # bins æ•°é‡
    k = 10     # hash å‡½æ•°æ•°é‡
    n = 100    # æˆ‘ä»¬æ’å…¥çš„é¡¹ç›®æ•°é‡

    text("è€ƒè™‘ä¸€ä¸ªæµ‹è¯•è¾“å…¥ï¼ˆä¸åœ¨é›†åˆä¸­ï¼‰ï¼Œå®ƒä¼š hash åˆ°ç»™å®šçš„æµ‹è¯• binï¼ˆæ¯”å¦‚ iï¼‰ã€‚")
    text("ç°åœ¨è€ƒè™‘å°†é¡¹ç›®æ”¾å…¥ Bloom filter å¹¶æŸ¥çœ‹å®ƒæ˜¯å¦å‘½ä¸­ iã€‚")

    # æ’å…¥ä¸€ä¸ªé¡¹ç›®ï¼Œè¯¢é—®æµ‹è¯• bin B(i) = 1ï¼Ÿ
    # B: [0 0 1 0 0 0 0 0 0 0] - å¿…é¡»é”™è¿‡ 1 æ¬¡
    f = 1 / m                              # P[B(i) = 1 after 1 insertion with 1 hash function]  # @inspect f
    # B: [0 0 1 0 0 1 0 1 0 0] - å¿…é¡»é”™è¿‡ k æ¬¡
    f = 1 - (1 - 1 / m) ** k               # P[B(i) = 1 after 1 insertion with k hash functions]  # @inspect f

    # æ’å…¥ n ä¸ªé¡¹ç›®ï¼Œè¯¢é—®æµ‹è¯• bin B(i) = 1ï¼Ÿ
    # å¿…é¡»é”™è¿‡ k*n æ¬¡
    f = 1 - (1 - 1 / m) ** (k * n)         # P[B(i) = 1 after n insertions for 1 hash function]  # @inspect f
    # æœ‰ k æ¬¡æœºä¼šé”™è¿‡ï¼ˆå› ä¸ºæµ‹è¯•è¾“å…¥ä¹Ÿè¢« hash k æ¬¡ï¼‰
    f = f ** k                             # P[B(i) = 1 after n insertions for k hash functions]  # @inspect f

    text("k çš„æœ€ä¼˜å€¼ï¼ˆç»™å®šå›ºå®šçš„ m / n æ¯”ç‡ï¼‰[ç»“æœ f ~ 0.5]")
    k = math.log(2) * m / n  # @inspect k
    text("æ”¹è¿›åçš„å‡é˜³æ€§ç‡")
    f = 0.5 ** k  # @inspect f

    text("è®¡ç®— (k)ã€å†…å­˜ (m) å’Œå‡é˜³æ€§ç‡ (f) ä¹‹é—´çš„æƒè¡¡ "), named_link("lecture notes", "https://people.eecs.berkeley.edu/~daw/teaching/cs170-s03/Notes/lecture10.pdf")

    text("ç¤ºä¾‹ï¼šDolma")
    text("- å°†å‡é˜³æ€§ç‡è®¾ç½®ä¸º 1e-15")
    text("- åœ¨é¡¹ç›® = æ®µè½ä¸Šæ‰§è¡Œ")


def build_table(items: list[str], num_bins: int):
    """æ„å»ºå¤§å°ä¸º `num_bins` çš„ Bloom filter è¡¨ï¼Œå°† `items` æ’å…¥å…¶ä¸­ã€‚"""
    table = bitarray(num_bins)  # @inspect table
    for item in items:
        h = mmh3.hash(item) % num_bins  # @inspect item, @inspect h
        table[h] = 1  # @inspect table
    return table


def build_table_k(items: list[str], num_bins: int, k: int):
    """æ„å»ºå¤§å°ä¸º `num_bins` çš„ Bloom filter è¡¨ï¼Œå°† `items` æ’å…¥å…¶ä¸­ã€‚
    ä½¿ç”¨ `k` ä¸ª hash å‡½æ•°ã€‚"""
    table = bitarray(num_bins)  # @inspect table
    for item in items:
        # å¯¹äº k ä¸ªå‡½æ•°ä¸­çš„æ¯ä¸€ä¸ª
        for seed in range(k):
            h = mmh3.hash(item, seed) % num_bins  # @inspect item, @inspect h, @inspect seed
            table[h] = 1  # @inspect table
    return table


def query_table(table: bitarray, item: str, num_bins: int, seed: int = 0):
    """è¿”å› `item` æ˜¯å¦åœ¨ `table` ä¸­ã€‚"""
    h = mmh3.hash(item, seed) % num_bins
    return table[h]


def query_table_k(table: bitarray, item: str, num_bins: int, k: int):
    """å¦‚æœæ‰€æœ‰ `k` ä¸ª hash å‡½æ•°çš„è¡¨éƒ½è®¾ç½®ä¸º 1ï¼Œåˆ™è¿”å› 1ã€‚"""
    return int(all(
        query_table(table, item, num_bins, seed)
        for seed in range(k)
    ))


def jaccard_minhash():
    text("ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿‘ä¼¼é›†åˆæˆå‘˜èµ„æ ¼ã€‚")
    text("é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç›¸ä¼¼åº¦åº¦é‡ã€‚")

    text("### Jaccard ç›¸ä¼¼åº¦")
    text("å®šä¹‰ï¼šJaccard(A, B) = |A intersect B| / |A union B|")
    A = {"1", "2", "3", "4"}
    B = {"1", "2", "3", "5"}

    def compute_jaccard(A, B):
        intersection = len(A & B)  # @inspect intersection
        union = len(A | B)  # @inspect union
        return intersection / union
    jaccard = compute_jaccard(A, B)  # @inspect jaccard

    text("å®šä¹‰ï¼šå¦‚æœä¸¤ä¸ªæ–‡æ¡£çš„ Jaccard ç›¸ä¼¼åº¦ >= é˜ˆå€¼ï¼Œåˆ™å®ƒä»¬æ˜¯**è¿‘ä¼¼é‡å¤**")

    text("ç®—æ³•æŒ‘æˆ˜ï¼šåœ¨çº¿æ€§æ—¶é—´å†…æ‰¾åˆ°è¿‘ä¼¼é‡å¤")

    text("### MinHash")
    text("MinHashï¼šä¸€ä¸ªéšæœº hash å‡½æ•° hï¼Œä½¿å¾— Pr[h(A) = h(B)] = Jaccard(A, B)")

    text("é€šå¸¸ï¼Œä½ å¸Œæœ›ä¸åŒçš„é¡¹ç›® hash åˆ°ä¸åŒçš„ hash")
    text("...ä½†åœ¨è¿™é‡Œï¼Œä½ å¸Œæœ›ç¢°æ’æ¦‚ç‡å–å†³äºç›¸ä¼¼åº¦")

    def minhash(S: set[str], seed: int):
        return min(mmh3.hash(x, seed) for x in S)

    text("ç‰¹å¾çŸ©é˜µè¡¨ç¤ºï¼š")
    text("item | A | B", verbatim=True)
    text("1    | 1 | 1", verbatim=True)
    text("2    | 1 | 1", verbatim=True)
    text("3    | 1 | 1", verbatim=True)
    text("4    | 1 | 0", verbatim=True)
    text("5    | 0 | 1", verbatim=True)

    text("éšæœº hash å‡½æ•°åœ¨é¡¹ç›®ä¸Šè¯±å¯¼ä¸€ä¸ªæ’åˆ—")
    text("æŸ¥çœ‹å“ªä¸ªé¡¹ç›®åœ¨ A ä¸­æ˜¯ç¬¬ä¸€ä¸ªï¼Œå“ªä¸ªé¡¹ç›®åœ¨ B ä¸­æ˜¯ç¬¬ä¸€ä¸ªã€‚")
    text("æ¯ä¸ªé¡¹ç›®æˆä¸ºç¬¬ä¸€ä¸ªï¼ˆminï¼‰çš„æ¦‚ç‡ç›¸åŒ")
    text("- å¦‚æœ 1ã€2ã€3 æ˜¯ç¬¬ä¸€ä¸ªï¼Œåˆ™ A ä¸­çš„ç¬¬ä¸€ä¸ª = B ä¸­çš„ç¬¬ä¸€ä¸ªã€‚")
    text("- å¦‚æœ 4ã€5 æ˜¯ç¬¬ä¸€ä¸ªï¼Œåˆ™ A ä¸­çš„ç¬¬ä¸€ä¸ª â‰  B ä¸­çš„ç¬¬ä¸€ä¸ªã€‚")

    # éªŒè¯ MinHash è¿‘ä¼¼ Jaccard
    n = 100  # ç”Ÿæˆè¿™ä¹ˆå¤šéšæœº hash å‡½æ•°
    matches = [minhash(A, seed) == minhash(B, seed) for seed in range(n)]
    estimated_jaccard = count(matches, True) / len(matches)  # @inspect estimated_jaccard
    assert abs(estimated_jaccard - jaccard) < 0.01

    text("ç°åœ¨æˆ‘ä»¬å¯ä»¥ hash æˆ‘ä»¬çš„é¡¹ç›®ï¼Œä½†ç¢°æ’å¹¶ä¸èƒ½å‘Šè¯‰æˆ‘ä»¬ Jaccard(A, B) > thresholdã€‚")


def locality_sensitive_hashing():
    text("å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ (LSH) "), named_link("book chapter", "http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf")

    text("å‡è®¾æˆ‘ä»¬åªç”¨ä¸€ä¸ª MinHash å‡½æ•°å¯¹ç¤ºä¾‹è¿›è¡Œ hash")
    text("P[A å’Œ B ç¢°æ’] = Jaccard(A, B)")
    text("å¹³å‡è€Œè¨€ï¼Œæ›´ç›¸ä¼¼çš„é¡¹ç›®ä¼šç¢°æ’ï¼Œä½†éå¸¸éšæœº...")

    text("ç›®æ ‡ï¼šå¦‚æœ Jaccard(A, B) > thresholdï¼Œåˆ™è®© A å’Œ B ç¢°æ’")
    text("æˆ‘ä»¬å¿…é¡»ä»¥æŸç§æ–¹å¼é”åŒ–æ¦‚ç‡...")

    text("è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ n ä¸ª hash å‡½æ•°")
    text("åˆ†è§£ä¸º b ä¸ª bandï¼Œæ¯ä¸ª band æœ‰ r ä¸ª hash å‡½æ•°ï¼ˆn = b * rï¼‰")

    n = 12      # hash å‡½æ•°æ•°é‡
    b = 3       # band æ•°é‡
    r = 4       # æ¯ä¸ª band çš„ hash å‡½æ•°æ•°é‡
    text("Hash å‡½æ•°ï¼š")
    text("h1 h2 h3 h4  |  h5 h6 h7 h8  |  h9 h10 h11 h12", verbatim=True)

    text("å…³é”®ï¼šå¦‚æœå¯¹äº*æŸä¸ª* bandï¼Œ*æ‰€æœ‰*å…¶ hash å‡½æ•°è¿”å›ç›¸åŒçš„å€¼ï¼Œåˆ™ A å’Œ B ç¢°æ’")
    text("æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œband çš„ä¸-æˆ–ç»“æ„é”åŒ–äº†é˜ˆå€¼")

    text("ç»™å®š Jaccard(A, B)ï¼ŒA å’Œ B ç¢°æ’çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ")

    def get_prob_collision(sim, b, r):  # @inspect sim, @inspect b, @inspect r
        prob_match = sim ** r                        # å›ºå®š band åŒ¹é…çš„æ¦‚ç‡  @inspect prob_match
        prob_collision = 1 - (1 - prob_match) ** b   # æŸä¸ª band åŒ¹é…çš„æ¦‚ç‡  @inspect prob_collision
        return prob_collision

    text("**ç¤ºä¾‹**")
    prob_collision = get_prob_collision(sim=0.8, b=5, r=10)  # @inspect prob_collision
    image("https://cdn.sanity.io/images/vr8gru94/production/b470799575b8e77911bacb8500977afef06d6c85-1280x720.png", width=600)


    sims = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.98]
    probs = {sim: get_prob_collision(sim=sim, b=10, r=10) for sim in sims}  # @inspect probs

    text("å¢åŠ  r é”åŒ–é˜ˆå€¼å¹¶å°†æ›²çº¿å‘å³ç§»åŠ¨ï¼ˆæ›´éš¾åŒ¹é…ï¼‰")
    probs = {sim: get_prob_collision(sim=sim, b=10, r=20) for sim in sims}  # @inspect probs

    text("å¢åŠ  b å°†æ›²çº¿å‘å·¦ç§»åŠ¨ï¼ˆæ›´å®¹æ˜“åŒ¹é…ï¼‰")
    probs = {sim: get_prob_collision(sim=sim, b=20, r=20) for sim in sims}  # @inspect probs
    image("https://cdn.sanity.io/images/vr8gru94/production/aace49fa240778e8ecf6e85ad08a2de7f5385566-1280x720.png", width=600)

    text("ç¤ºä¾‹è®¾ç½® "), link("https://arxiv.org/pdf/2107.06499"), text("ï¼šn = 9000, b = 20, r = 450")
    b = 20
    r = 450
    text("é˜ˆå€¼æ˜¯å¤šå°‘ï¼ˆç›¸å˜å‘ç”Ÿçš„åœ°æ–¹ï¼‰ï¼Ÿ")
    threshold = (1 / b) ** (1 / r)  # @inspect threshold
    text("å›ºå®š band åŒ¹é…çš„æ¦‚ç‡ï¼š")
    prob_match = (1 / b)  # @inspect prob_match
    text("A å’Œ B ç¢°æ’çš„æ¦‚ç‡ï¼ˆâ‰ˆ 1-1/eï¼‰ï¼š")
    prob_collision = 1 - (1 - 1 / b) ** b  #  @inspect prob_collision


if __name__ == "__main__":
    main()
